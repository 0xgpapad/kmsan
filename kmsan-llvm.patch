Index: include/llvm/Transforms/Instrumentation.h
===================================================================
--- include/llvm/Transforms/Instrumentation.h	(revision 320373)
+++ include/llvm/Transforms/Instrumentation.h	(working copy)
@@ -131,7 +131,8 @@
 
 // Insert MemorySanitizer instrumentation (detection of uninitialized reads)
 FunctionPass *createMemorySanitizerPass(int TrackOrigins = 0,
-                                        bool Recover = false);
+                                        bool Recover = false,
+                                        bool EnableKmsan = false);
 
 FunctionPass *createHWAddressSanitizerPass();
 
Index: lib/Transforms/Instrumentation/MemorySanitizer.cpp
===================================================================
--- lib/Transforms/Instrumentation/MemorySanitizer.cpp	(revision 320373)
+++ lib/Transforms/Instrumentation/MemorySanitizer.cpp	(working copy)
@@ -89,7 +89,29 @@
 /// implementation ignores the load aspect of CAS/RMW, always returning a clean
 /// value. It implements the store part as a simple atomic store by storing a
 /// clean shadow.
-//
+///
+///                  KernelMemorySanitizer (KMSAN) implementation.
+///
+/// The major differences between KMSAN and MSan instrumentation are:
+///  - KMSAN implies msan-track-origins=2, msan-keep-going=true;
+///  - KMSAN allocates shadows and origins for each page separately, so there
+///    are no explicit accesses to shadow and origin memory.
+///    Shadow and origin values for a particular X-byte memory location
+///    (X=1,2,4,8) are accessed via pointers obtained via the
+///      __msan_metadata_ptr_for_load_X(ptr)
+///      __msan_metadata_ptr_for_store_X(ptr)
+///    functions. The corresponding functions check that the X-byte accesses
+///    are possible and returns the pointers to shadow and origin memory.
+///      __msan_metadata_ptr_for_load_n
+///      __msan_metadata_ptr_for_store_X
+///  - TLS variables are stored in a single struct in per-task storage. A call
+///    to a function __msan_get_context_state() returning a pointer to that
+///    struct is inserted into every instrumented function before the entry
+///    block;
+///  - __msan_warning() now becomes __msan_warning_32(uptr origin)
+///
+///  KernelMemorySanitizer only supports X86_64 at the moment.
+///
 //===----------------------------------------------------------------------===//
 
 #include "llvm/ADT/APInt.h"
@@ -199,6 +221,10 @@
        cl::desc("exact handling of relational integer ICmp"),
        cl::Hidden, cl::init(false));
 
+static cl::opt<bool> ClEnableKmsan(
+    "msan-kernel", cl::desc("Enable KernelMemorySanitizer instrumentation"),
+    cl::Hidden, cl::init(false));
+
 // This flag controls whether we check the shadow of the address
 // operand of load or store. Such bugs are very rare, since load from
 // a garbage address typically results in SEGV, but still happen
@@ -368,17 +394,20 @@
 class MemorySanitizer : public FunctionPass {
 public:
   // Pass identification, replacement for typeid.
-  static char ID; 
+ static char ID;
 
-  MemorySanitizer(int TrackOrigins = 0, bool Recover = false)
-      : FunctionPass(ID),
-        TrackOrigins(std::max(TrackOrigins, (int)ClTrackOrigins)),
-        Recover(Recover || ClKeepGoing) {}
+ MemorySanitizer(int TrackOrigins = 0, bool Recover = false,
+                 bool EnableKmsan = false)
+     : FunctionPass(ID),
+       CompileKernel(EnableKmsan || ClEnableKmsan),
+       TrackOrigins(
+           CompileKernel ? 2 : std::max(TrackOrigins, (int)ClTrackOrigins)),
+       Recover(Recover || ClKeepGoing || CompileKernel),
+       WarningFn(nullptr) {}
+ StringRef getPassName() const override { return "MemorySanitizer"; }
 
-  StringRef getPassName() const override { return "MemorySanitizer"; }
-
-  void getAnalysisUsage(AnalysisUsage &AU) const override {
-    AU.addRequired<TargetLibraryInfoWrapperPass>();
+ void getAnalysisUsage(AnalysisUsage &AU) const override {
+   AU.addRequired<TargetLibraryInfoWrapperPass>();
   }
 
   bool runOnFunction(Function &F) override;
@@ -392,7 +421,12 @@
   friend struct VarArgPowerPC64Helper;
 
   void initializeCallbacks(Module &M);
+  void createKernelApi(Module &M);
+  void createUserspaceApi(Module &M);
 
+  /// \brief True if we're compiling the Linux kernel.
+  bool CompileKernel;
+
   /// \brief Track origins (allocation points) of uninitialized values.
   int TrackOrigins;
   bool Recover;
@@ -402,32 +436,38 @@
   Type *OriginTy;
 
   /// \brief Thread-local shadow storage for function parameters.
-  GlobalVariable *ParamTLS;
+  Value *ParamTLS;
 
   /// \brief Thread-local origin storage for function parameters.
-  GlobalVariable *ParamOriginTLS;
+  Value *ParamOriginTLS;
 
   /// \brief Thread-local shadow storage for function return value.
-  GlobalVariable *RetvalTLS;
+  Value *RetvalTLS;
 
   /// \brief Thread-local origin storage for function return value.
-  GlobalVariable *RetvalOriginTLS;
+  Value *RetvalOriginTLS;
 
   /// \brief Thread-local shadow storage for in-register va_arg function
   /// parameters (x86_64-specific).
-  GlobalVariable *VAArgTLS;
+  Value *VAArgTLS;
 
+  // \brief Thread-local shadow storage for in-register va_arg function
+  // parameters (x86_64-specific, KMSAN only).
+  Value *VAArgOriginTLS;
+
   /// \brief Thread-local shadow storage for va_arg overflow area
   /// (x86_64-specific).
-  GlobalVariable *VAArgOverflowSizeTLS;
+  Value *VAArgOverflowSizeTLS;
 
   /// \brief Thread-local space used to pass origin value to the UMR reporting
   /// function.
-  GlobalVariable *OriginTLS;
+  Value *OriginTLS;
 
   /// \brief The run-time callback to print a warning.
   Value *WarningFn = nullptr;
 
+  // \brief KMSAN-specific error callback that takes the origin.
+  Value *MsanWarning32Fn;
   // These arrays are indexed by log2(AccessSize).
   Value *MaybeWarningFn[kNumberOfAccessSizes];
   Value *MaybeStoreOriginFn[kNumberOfAccessSizes];
@@ -436,6 +476,14 @@
   /// allocation.
   Value *MsanSetAllocaOrigin4Fn;
 
+  Value *getKmsanShadowOriginAccessFn(bool isStore, int size);
+
+  Value *MsanPoisonAllocaFn;
+  Value *MsanUnpoisonFn;
+  Value *MsanLoadArgShadowFn, *MsanLoadArgOriginFn;
+  Value *MsanStoreArgShadowFn;
+  Value *MsanStoreArgShadowOriginFn;
+
   /// \brief Run-time helper that poisons stack on function entry.
   Value *MsanPoisonStackFn;
 
@@ -446,6 +494,14 @@
   /// \brief MSan runtime replacements for memmove, memcpy and memset.
   Value *MemmoveFn, *MemcpyFn, *MemsetFn;
 
+  /// \brief KMSAN callback for task-local function argument shadow.
+  Value *GetContextStateFn;
+
+  /// \brief function returning a pair of shadow/origin pointers.
+  Value *MsanMetadataPtrForLoadN, *MsanMetadataPtrForStoreN;
+  Value *MsanMetadataPtrForLoad_1_8[4];
+  Value *MsanMetadataPtrForStore_1_8[4];
+
   /// \brief Memory map parameters used in application-to-shadow calculation.
   const MemoryMapParams *MapParams;
 
@@ -472,8 +528,9 @@
     MemorySanitizer, "msan",
     "MemorySanitizer: detects uninitialized reads.", false, false)
 
-FunctionPass *llvm::createMemorySanitizerPass(int TrackOrigins, bool Recover) {
-  return new MemorySanitizer(TrackOrigins, Recover);
+FunctionPass *llvm::createMemorySanitizerPass(int TrackOrigins, bool Recover,
+                                              bool CompileKernel) {
+  return new MemorySanitizer(TrackOrigins, Recover, CompileKernel);
 }
 
 /// \brief Create a non-const global initialized with the given string.
@@ -488,13 +545,91 @@
                             GlobalValue::PrivateLinkage, StrConst, "");
 }
 
-/// \brief Insert extern declaration of runtime-provided functions and globals.
-void MemorySanitizer::initializeCallbacks(Module &M) {
-  // Only do this once.
-  if (WarningFn)
-    return;
+/// \brief Create KMSAN API callbacks.
+void MemorySanitizer::createKernelApi(Module &M) {
+  IRBuilder<> IRB(*C);
 
+  // These will be initialized in insertKmsanPrologue().
+  RetvalTLS = nullptr;
+  RetvalOriginTLS = nullptr;
+  ParamTLS = nullptr;
+  ParamOriginTLS = nullptr;
+  VAArgTLS = nullptr;
+  VAArgOriginTLS = nullptr;
+  VAArgOverflowSizeTLS = nullptr;
+  // OriginTLS is unused in the kernel.
+  OriginTLS = nullptr;
+
+  // Like __msan_warning(), but takes an origin.
+  MsanWarning32Fn = M.getOrInsertFunction("__msan_warning_32", IRB.getVoidTy(),
+                                          IRB.getInt32Ty());
+  // Requests the per-task context state (kmsan_context_state*) from the
+  // runtime library.
+  GetContextStateFn = M.getOrInsertFunction(
+      "__msan_get_context_state",
+      PointerType::get(
+          StructType::get(ArrayType::get(IRB.getInt64Ty(), kParamTLSSize / 8),
+                          ArrayType::get(IRB.getInt64Ty(), kRetvalTLSSize / 8),
+                          ArrayType::get(IRB.getInt64Ty(), kParamTLSSize / 8),
+                          ArrayType::get(IRB.getInt64Ty(),
+                                         kParamTLSSize / 8), /* va_arg_origin */
+                          IRB.getInt64Ty(),
+                          ArrayType::get(OriginTy, kParamTLSSize / 4), OriginTy,
+                          OriginTy),
+          0));
+
+  Type *RetTy = StructType::get(PointerType::get(IRB.getInt8Ty(), 0),
+                                PointerType::get(IRB.getInt32Ty(), 0));
+
+  for (int ind = 0, size = 1; ind < 4; ind++, size <<= 1) {
+    std::string name_load =
+        "__msan_metadata_ptr_for_load_" + std::to_string(size);
+    std::string name_store =
+        "__msan_metadata_ptr_for_store_" + std::to_string(size);
+    MsanMetadataPtrForLoad_1_8[ind] = M.getOrInsertFunction(
+        name_load, RetTy, PointerType::get(IRB.getInt8Ty(), 0));
+    MsanMetadataPtrForStore_1_8[ind] = M.getOrInsertFunction(
+        name_store, RetTy, PointerType::get(IRB.getInt8Ty(), 0));
+  }
+
+  MsanMetadataPtrForLoadN = M.getOrInsertFunction(
+      "__msan_metadata_ptr_for_load_n", RetTy,
+      PointerType::get(IRB.getInt8Ty(), 0), IRB.getInt64Ty());
+  MsanMetadataPtrForStoreN = M.getOrInsertFunction(
+      "__msan_metadata_ptr_for_store_n", RetTy,
+      PointerType::get(IRB.getInt8Ty(), 0), IRB.getInt64Ty());
+
+  // Functions for poisoning and unpoisoning memory.
+  MsanPoisonAllocaFn = M.getOrInsertFunction(
+      "__msan_poison_alloca", IRB.getVoidTy(), IRB.getInt8PtrTy(), IntptrTy,
+      IRB.getInt8PtrTy(), IntptrTy);
+
+  MsanUnpoisonFn = M.getOrInsertFunction("__msan_unpoison", IRB.getVoidTy(),
+                                         IRB.getInt8PtrTy(), IntptrTy);
+
+  // Functions for loading the shadow for a given memory range into an app
+  // memory buffer and storing it back to the shadow memory.
+  MsanLoadArgShadowFn =
+      M.getOrInsertFunction("__msan_load_arg_shadow", IRB.getVoidTy(),
+                            IRB.getInt8PtrTy(), IRB.getInt8PtrTy(), IntptrTy);
+  MsanLoadArgOriginFn =
+      M.getOrInsertFunction("__msan_load_arg_origin", IRB.getVoidTy(),
+                            IRB.getInt8PtrTy(), IRB.getInt8PtrTy(), IntptrTy);
+
+  MsanStoreArgShadowFn =
+      M.getOrInsertFunction("__msan_store_arg_shadow", IRB.getVoidTy(),
+                            IRB.getInt8PtrTy(), IRB.getInt8PtrTy(), IntptrTy);
+
+  // Functions for loading the origin for a given memory range into an app
+  // memory buffer and storing it back to the origin memory.
+  MsanStoreArgShadowOriginFn = M.getOrInsertFunction(
+      "__msan_store_arg_shadow_origin", IRB.getVoidTy(), IRB.getInt8PtrTy(),
+      IRB.getInt8PtrTy(), IRB.getInt8PtrTy(), IntptrTy);
+}
+
+void MemorySanitizer::createUserspaceApi(Module &M) {
   IRBuilder<> IRB(*C);
+
   // Create the callback.
   // FIXME: this function should have "Cold" calling conv,
   // which is not yet implemented.
@@ -502,6 +637,39 @@
                                     : "__msan_warning_noreturn";
   WarningFn = M.getOrInsertFunction(WarningFnName, IRB.getVoidTy());
 
+  // Create the global TLS variables.
+  RetvalTLS = new GlobalVariable(
+      M, ArrayType::get(IRB.getInt64Ty(), kRetvalTLSSize / 8), false,
+      GlobalVariable::ExternalLinkage, nullptr, "__msan_retval_tls", nullptr,
+      GlobalVariable::InitialExecTLSModel);
+
+  RetvalOriginTLS = new GlobalVariable(
+      M, OriginTy, false, GlobalVariable::ExternalLinkage, nullptr,
+      "__msan_retval_origin_tls", nullptr, GlobalVariable::InitialExecTLSModel);
+
+  ParamTLS = new GlobalVariable(
+      M, ArrayType::get(IRB.getInt64Ty(), kParamTLSSize / 8), false,
+      GlobalVariable::ExternalLinkage, nullptr, "__msan_param_tls", nullptr,
+      GlobalVariable::InitialExecTLSModel);
+
+  ParamOriginTLS = new GlobalVariable(
+      M, ArrayType::get(OriginTy, kParamTLSSize / 4), false,
+      GlobalVariable::ExternalLinkage, nullptr, "__msan_param_origin_tls",
+      nullptr, GlobalVariable::InitialExecTLSModel);
+
+  VAArgTLS = new GlobalVariable(
+      M, ArrayType::get(IRB.getInt64Ty(), kParamTLSSize / 8), false,
+      GlobalVariable::ExternalLinkage, nullptr, "__msan_va_arg_tls", nullptr,
+      GlobalVariable::InitialExecTLSModel);
+  VAArgOverflowSizeTLS = new GlobalVariable(
+      M, IRB.getInt64Ty(), false, GlobalVariable::ExternalLinkage, nullptr,
+      "__msan_va_arg_overflow_size_tls", nullptr,
+      GlobalVariable::InitialExecTLSModel);
+  OriginTLS = new GlobalVariable(
+      M, IRB.getInt32Ty(), false, GlobalVariable::ExternalLinkage, nullptr,
+      "__msan_origin_tls", nullptr, GlobalVariable::InitialExecTLSModel);
+
+  // Create the _maybe_ functions.
   for (size_t AccessSizeIndex = 0; AccessSizeIndex < kNumberOfAccessSizes;
        AccessSizeIndex++) {
     unsigned AccessSize = 1 << AccessSizeIndex;
@@ -516,6 +684,7 @@
         IRB.getInt8PtrTy(), IRB.getInt32Ty());
   }
 
+  // Functions for stack instrumentation.
   MsanSetAllocaOrigin4Fn = M.getOrInsertFunction(
     "__msan_set_alloca_origin4", IRB.getVoidTy(), IRB.getInt8PtrTy(), IntptrTy,
     IRB.getInt8PtrTy(), IntptrTy);
@@ -522,6 +691,15 @@
   MsanPoisonStackFn =
       M.getOrInsertFunction("__msan_poison_stack", IRB.getVoidTy(),
                             IRB.getInt8PtrTy(), IntptrTy);
+}
+
+/// \brief Insert extern declaration of runtime-provided functions and globals.
+void MemorySanitizer::initializeCallbacks(Module &M) {
+  // Only do this once.
+  static bool CallbacksInitialized = false;
+  if (CallbacksInitialized) return;
+
+  IRBuilder<> IRB(*C);
   MsanChainOriginFn = M.getOrInsertFunction(
     "__msan_chain_origin", IRB.getInt32Ty(), IRB.getInt32Ty());
   MemmoveFn = M.getOrInsertFunction(
@@ -534,42 +712,36 @@
     "__msan_memset", IRB.getInt8PtrTy(), IRB.getInt8PtrTy(), IRB.getInt32Ty(),
     IntptrTy);
 
-  // Create globals.
-  RetvalTLS = new GlobalVariable(
-    M, ArrayType::get(IRB.getInt64Ty(), kRetvalTLSSize / 8), false,
-    GlobalVariable::ExternalLinkage, nullptr, "__msan_retval_tls", nullptr,
-    GlobalVariable::InitialExecTLSModel);
-  RetvalOriginTLS = new GlobalVariable(
-    M, OriginTy, false, GlobalVariable::ExternalLinkage, nullptr,
-    "__msan_retval_origin_tls", nullptr, GlobalVariable::InitialExecTLSModel);
+  if (!CompileKernel) {
+    createUserspaceApi(M);
+  } else {
+    createKernelApi(M);
+  }
 
-  ParamTLS = new GlobalVariable(
-    M, ArrayType::get(IRB.getInt64Ty(), kParamTLSSize / 8), false,
-    GlobalVariable::ExternalLinkage, nullptr, "__msan_param_tls", nullptr,
-    GlobalVariable::InitialExecTLSModel);
-  ParamOriginTLS = new GlobalVariable(
-    M, ArrayType::get(OriginTy, kParamTLSSize / 4), false,
-    GlobalVariable::ExternalLinkage, nullptr, "__msan_param_origin_tls",
-    nullptr, GlobalVariable::InitialExecTLSModel);
-
-  VAArgTLS = new GlobalVariable(
-    M, ArrayType::get(IRB.getInt64Ty(), kParamTLSSize / 8), false,
-    GlobalVariable::ExternalLinkage, nullptr, "__msan_va_arg_tls", nullptr,
-    GlobalVariable::InitialExecTLSModel);
-  VAArgOverflowSizeTLS = new GlobalVariable(
-    M, IRB.getInt64Ty(), false, GlobalVariable::ExternalLinkage, nullptr,
-    "__msan_va_arg_overflow_size_tls", nullptr,
-    GlobalVariable::InitialExecTLSModel);
-  OriginTLS = new GlobalVariable(
-    M, IRB.getInt32Ty(), false, GlobalVariable::ExternalLinkage, nullptr,
-    "__msan_origin_tls", nullptr, GlobalVariable::InitialExecTLSModel);
-
   // We insert an empty inline asm after __msan_report* to avoid callback merge.
   EmptyAsm = InlineAsm::get(FunctionType::get(IRB.getVoidTy(), false),
                             StringRef(""), StringRef(""),
                             /*hasSideEffects=*/true);
+  CallbacksInitialized = true;
 }
 
+Value *MemorySanitizer::getKmsanShadowOriginAccessFn(bool isStore, int size) {
+  Value **Fns =
+      isStore ? MsanMetadataPtrForStore_1_8 : MsanMetadataPtrForLoad_1_8;
+  switch (size) {
+    case 1:
+      return Fns[0];
+    case 2:
+      return Fns[1];
+    case 4:
+      return Fns[2];
+    case 8:
+      return Fns[3];
+    default:
+      return nullptr;
+  }
+}
+
 /// \brief Module-level initialization.
 ///
 /// inserts a call to __msan_init to the module's constructor list.
@@ -639,12 +811,14 @@
       createSanitizerCtorAndInitFunctions(M, kMsanModuleCtorName, kMsanInitName,
                                           /*InitArgTypes=*/{},
                                           /*InitArgs=*/{});
-  if (ClWithComdat) {
-    Comdat *MsanCtorComdat = M.getOrInsertComdat(kMsanModuleCtorName);
-    MsanCtorFunction->setComdat(MsanCtorComdat);
-    appendToGlobalCtors(M, MsanCtorFunction, 0, MsanCtorFunction);
-  } else {
-    appendToGlobalCtors(M, MsanCtorFunction, 0);
+  if (!CompileKernel) {
+    if (ClWithComdat) {
+      Comdat *MsanCtorComdat = M.getOrInsertComdat(kMsanModuleCtorName);
+      MsanCtorFunction->setComdat(MsanCtorComdat);
+      appendToGlobalCtors(M, MsanCtorFunction, 0, MsanCtorFunction);
+    } else {
+      appendToGlobalCtors(M, MsanCtorFunction, 0);
+    }
   }
 
 
@@ -715,6 +889,7 @@
   ValueMap<Value*, Value*> ShadowMap, OriginMap;
   std::unique_ptr<VarArgHelper> VAHelper;
   const TargetLibraryInfo *TLI;
+  BasicBlock *ActualFnStart;
 
   // The following flags disable parts of MSan instrumentation based on
   // blacklist contents and command-line options.
@@ -819,7 +994,7 @@
       unsigned TypeSizeInBits =
           DL.getTypeSizeInBits(ConvertedShadow->getType());
       unsigned SizeIndex = TypeSizeToSizeIndex(TypeSizeInBits);
-      if (AsCall && SizeIndex < kNumberOfAccessSizes) {
+      if (AsCall && SizeIndex < kNumberOfAccessSizes && !MS.CompileKernel) {
         Value *Fn = MS.MaybeStoreOriginFn[SizeIndex];
         Value *ConvertedShadow2 = IRB.CreateZExt(
             ConvertedShadow, IRB.getIntNTy(8 * (1 << SizeIndex)));
@@ -838,6 +1013,14 @@
     }
   }
 
+  void maybeInsertAddressCheck(Value *Val, Instruction *OrigIns) {
+    Type *Ty = Val->getType();
+    PointerType *PtrTy = dyn_cast<PointerType>(Ty);
+    ///errs() << "Ptr: " << *PtrTy << "\n";
+    if (ClCheckAccessAddress)
+      insertShadowCheck(Val, OrigIns);
+  }
+
   void materializeStores(bool InstrumentWithCalls) {
     for (StoreInst *SI : StoreList) {
       IRBuilder<> IRB(SI);
@@ -848,14 +1031,13 @@
       Type *ShadowTy = Shadow->getType();
       unsigned Alignment = SI->getAlignment();
       unsigned OriginAlignment = std::max(kMinOriginAlignment, Alignment);
-      std::tie(ShadowPtr, OriginPtr) =
-          getShadowOriginPtr(Addr, IRB, ShadowTy, Alignment);
+      std::tie(ShadowPtr, OriginPtr) = getShadowOriginPtr(
+          Addr, IRB, ShadowTy, Alignment, /*isStore*/ true);
 
       StoreInst *NewSI = IRB.CreateAlignedStore(Shadow, ShadowPtr, Alignment);
       DEBUG(dbgs() << "  STORE: " << *NewSI << "\n");
 
-      if (ClCheckAccessAddress)
-        insertShadowCheck(Addr, NewSI);
+      maybeInsertAddressCheck(Addr, NewSI);
 
       if (SI->isAtomic())
         SI->setOrdering(addReleaseOrdering(SI->getOrdering()));
@@ -866,6 +1048,23 @@
     }
   }
 
+/// \brief Helper function to insert a warning at IRB's current insert point.
+void insertWarningFn(IRBuilder<> &IRB, Value *Origin) {
+  if (!Origin) Origin = (Value *)IRB.getInt32(0);
+  if (!MS.CompileKernel) {
+    if (MS.TrackOrigins) {
+      IRB.CreateStore(Origin, MS.OriginTLS);
+    }
+    IRB.CreateCall(MS.WarningFn, {});
+  } else {
+    IRB.CreateCall(MS.MsanWarning32Fn, Origin);
+  }
+  IRB.CreateCall(MS.EmptyAsm, {});
+  // FIXME: Insert UnreachableInst if !MS.Recover?
+  // This may invalidate some of the following checks and needs to be done
+  // at the very end.
+}
+
   void materializeOneCheck(Instruction *OrigIns, Value *Shadow, Value *Origin,
                            bool AsCall) {
     IRBuilder<> IRB(OrigIns);
@@ -876,15 +1075,7 @@
     Constant *ConstantShadow = dyn_cast_or_null<Constant>(ConvertedShadow);
     if (ConstantShadow) {
       if (ClCheckConstantShadow && !ConstantShadow->isZeroValue()) {
-        if (MS.TrackOrigins) {
-          IRB.CreateStore(Origin ? (Value *)Origin : (Value *)IRB.getInt32(0),
-                          MS.OriginTLS);
-        }
-        IRB.CreateCall(MS.WarningFn, {});
-        IRB.CreateCall(MS.EmptyAsm, {});
-        // FIXME: Insert UnreachableInst if !MS.Recover?
-        // This may invalidate some of the following checks and needs to be done
-        // at the very end.
+        insertWarningFn(IRB, Origin);
       }
       return;
     }
@@ -893,7 +1084,7 @@
 
     unsigned TypeSizeInBits = DL.getTypeSizeInBits(ConvertedShadow->getType());
     unsigned SizeIndex = TypeSizeToSizeIndex(TypeSizeInBits);
-    if (AsCall && SizeIndex < kNumberOfAccessSizes) {
+    if (AsCall && SizeIndex < kNumberOfAccessSizes && !MS.CompileKernel) {
       Value *Fn = MS.MaybeWarningFn[SizeIndex];
       Value *ConvertedShadow2 =
           IRB.CreateZExt(ConvertedShadow, IRB.getIntNTy(8 * (1 << SizeIndex)));
@@ -908,12 +1099,7 @@
           /* Unreachable */ !MS.Recover, MS.ColdCallWeights);
 
       IRB.SetInsertPoint(CheckTerm);
-      if (MS.TrackOrigins) {
-        IRB.CreateStore(Origin ? (Value *)Origin : (Value *)IRB.getInt32(0),
-                        MS.OriginTLS);
-      }
-      IRB.CreateCall(MS.WarningFn, {});
-      IRB.CreateCall(MS.EmptyAsm, {});
+      insertWarningFn(IRB, Origin);
       DEBUG(dbgs() << "  CHECK: " << *Cmp << "\n");
     }
   }
@@ -928,9 +1114,36 @@
     DEBUG(dbgs() << "DONE:\n" << F);
   }
 
+  BasicBlock *insertKmsanPrologue(Function &F) {
+    BasicBlock *ret =
+        SplitBlock(&F.getEntryBlock(), F.getEntryBlock().getFirstNonPHI());
+    IRBuilder<> IRB(F.getEntryBlock().getFirstNonPHI());
+    Value *ContextState = IRB.CreateCall(MS.GetContextStateFn, {});
+    Constant *Zero = IRB.getInt32(0);
+    MS.ParamTLS =
+        IRB.CreateGEP(ContextState, {Zero, IRB.getInt32(0)}, "param_shadow");
+    MS.RetvalTLS =
+        IRB.CreateGEP(ContextState, {Zero, IRB.getInt32(1)}, "retval_shadow");
+    MS.VAArgTLS =
+        IRB.CreateGEP(ContextState, {Zero, IRB.getInt32(2)}, "va_arg_shadow");
+    MS.VAArgOriginTLS =
+        IRB.CreateGEP(ContextState, {Zero, IRB.getInt32(3)}, "va_arg_origin");
+    MS.VAArgOverflowSizeTLS = IRB.CreateGEP(
+        ContextState, {Zero, IRB.getInt32(4)}, "va_arg_overflow_size");
+    MS.ParamOriginTLS =
+        IRB.CreateGEP(ContextState, {Zero, IRB.getInt32(5)}, "param_origin");
+    MS.RetvalOriginTLS =
+        IRB.CreateGEP(ContextState, {Zero, IRB.getInt32(6)}, "retval_origin");
+    return ret;
+  }
+
   /// \brief Add MemorySanitizer instrumentation to a function.
   bool runOnFunction() {
     MS.initializeCallbacks(*F.getParent());
+    if (MS.CompileKernel)
+      ActualFnStart = insertKmsanPrologue(F);
+    else
+      ActualFnStart = &F.getEntryBlock();
 
     // In the presence of unreachable blocks, we may see Phi nodes with
     // incoming nodes from such blocks. Since InstVisitor skips unreachable
@@ -941,8 +1154,7 @@
     // Iterate all BBs in depth-first order and create shadow instructions
     // for all instructions (where applicable).
     // For PHI nodes we create dummy shadow PHIs which will be finalized later.
-    for (BasicBlock *BB : depth_first(&F.getEntryBlock()))
-      visit(*BB);
+    for (BasicBlock *BB : depth_first(ActualFnStart)) visit(*BB);
 
     // Finalize PHI nodes.
     for (PHINode *PN : ShadowPHINodes) {
@@ -1047,12 +1259,10 @@
   /// Shadow = ShadowBase + Offset
   /// Origin = (OriginBase + Offset) & ~3ULL
   std::pair<Value *, Value *> getShadowOriginPtrUserspace(
-      Value *Addr, IRBuilder<> &IRB, Type *ShadowTy, unsigned Alignment,
-      Instruction **FirstInsn) {
+      Value *Addr, IRBuilder<> &IRB, Type *ShadowTy, unsigned Alignment) {
     Value *ShadowOffset = getShadowPtrOffset(Addr, IRB);
     Value *ShadowLong = ShadowOffset;
     uint64_t ShadowBase = MS.MapParams->ShadowBase;
-    *FirstInsn = dyn_cast<Instruction>(ShadowLong);
     if (ShadowBase != 0) {
       ShadowLong =
         IRB.CreateAdd(ShadowLong,
@@ -1078,12 +1288,40 @@
     return std::make_pair(ShadowPtr, OriginPtr);
   }
 
+  std::pair<Value *, Value *> getShadowOriginPtrKernel(
+      Value *Addr, IRBuilder<> &IRB, Type *ShadowTy, unsigned Alignment,
+      bool isStore) {
+    Value *ShadowOriginPtrs;
+    int BitWidth = VectorOrPrimitiveTypeSizeInBits(ShadowTy);
+    int Size = BitWidth / 8;
+    // Make sure Size is at least 1 if the operand is i1.
+    if (Size * 8 < BitWidth) Size++;
+    Value *Getter = MS.getKmsanShadowOriginAccessFn(isStore, Size);
+    Value *AddrCast =
+        IRB.CreatePointerCast(Addr, PointerType::get(IRB.getInt8Ty(), 0));
+    if (Getter) {
+      ShadowOriginPtrs = IRB.CreateCall(Getter, AddrCast);
+    } else {
+      Value *SizeVal = ConstantInt::get(MS.IntptrTy, Size);
+      ShadowOriginPtrs =
+          IRB.CreateCall(MS.MsanMetadataPtrForLoadN, {AddrCast, SizeVal});
+    }
+    Value *ShadowPtr = IRB.CreateExtractValue(ShadowOriginPtrs, 0);
+    ShadowPtr = IRB.CreatePointerCast(ShadowPtr, PointerType::get(ShadowTy, 0));
+    Value *OriginPtr = IRB.CreateExtractValue(ShadowOriginPtrs, 1);
+    return std::make_pair(ShadowPtr, OriginPtr);
+  }
+
   std::pair<Value *, Value *> getShadowOriginPtr(Value *Addr, IRBuilder<> &IRB,
                                                  Type *ShadowTy,
-                                                 unsigned Alignment) {
-    Instruction *FirstInsn = nullptr;
-    std::pair<Value *, Value *> ret =
-        getShadowOriginPtrUserspace(Addr, IRB, ShadowTy, Alignment, &FirstInsn);
+                                                 unsigned Alignment,
+                                                 bool isStore) {
+    std::pair<Value *, Value *> ret;
+    if (!MS.CompileKernel) {
+      ret = getShadowOriginPtrUserspace(Addr, IRB, ShadowTy, Alignment);
+    } else {
+      ret = getShadowOriginPtrKernel(Addr, IRB, ShadowTy, Alignment, isStore);
+    }
     return ret;
   }
 
@@ -1093,7 +1331,8 @@
   Value *getShadowPtrForArgument(Value *A, IRBuilder<> &IRB,
                                  int ArgOffset) {
     Value *Base = IRB.CreatePointerCast(MS.ParamTLS, MS.IntptrTy);
-    Base = IRB.CreateAdd(Base, ConstantInt::get(MS.IntptrTy, ArgOffset));
+    if (ArgOffset)
+        Base = IRB.CreateAdd(Base, ConstantInt::get(MS.IntptrTy, ArgOffset));
     return IRB.CreateIntToPtr(Base, PointerType::get(getShadowTy(A), 0),
                               "_msarg");
   }
@@ -1103,7 +1342,8 @@
                                  int ArgOffset) {
     if (!MS.TrackOrigins) return nullptr;
     Value *Base = IRB.CreatePointerCast(MS.ParamOriginTLS, MS.IntptrTy);
-    Base = IRB.CreateAdd(Base, ConstantInt::get(MS.IntptrTy, ArgOffset));
+    if (ArgOffset)
+        Base = IRB.CreateAdd(Base, ConstantInt::get(MS.IntptrTy, ArgOffset));
     return IRB.CreateIntToPtr(Base, PointerType::get(MS.OriginTy, 0),
                               "_msarg_o");
   }
@@ -1212,7 +1452,7 @@
       if (*ShadowPtr)
         return *ShadowPtr;
       Function *F = A->getParent();
-      IRBuilder<> EntryIRB(F->getEntryBlock().getFirstNonPHI());
+      IRBuilder<> EntryIRB(ActualFnStart->getFirstNonPHI());
       unsigned ArgOffset = 0;
       const DataLayout &DL = F->getParent()->getDataLayout();
       for (auto &FArg : F->args()) {
@@ -1236,9 +1476,11 @@
               Type *EltType = A->getType()->getPointerElementType();
               ArgAlign = DL.getABITypeAlignment(EltType);
             }
-            Value *CpShadowPtr =
-                getShadowOriginPtr(V, EntryIRB, EntryIRB.getInt8Ty(), ArgAlign)
-                    .first;
+            Value *CpShadowPtr, *CpOriginPtr;
+            std::tie(CpShadowPtr, CpOriginPtr) =
+                getShadowOriginPtr(V, EntryIRB, EntryIRB.getInt8Ty(), ArgAlign,
+                                   /*isStore*/ true);
+            // TODO(glider): origins?
             if (Overflow) {
               // ParamTLS overflow.
               EntryIRB.CreateMemSet(
@@ -1396,14 +1638,13 @@
     unsigned Alignment = I.getAlignment();
     if (PropagateShadow) {
       std::tie(ShadowPtr, OriginPtr) =
-          getShadowOriginPtr(Addr, IRB, ShadowTy, Alignment);
-      setShadow(&I, IRB.CreateAlignedLoad(ShadowPtr, Alignment, "_msld"));
+          getShadowOriginPtr(Addr, IRB, ShadowTy, Alignment, /*isStore*/ false);
+      setShadow(&I, IRB.CreateAlignedLoad(ShadowPtr, I.getAlignment(), "_msld"));
     } else {
       setShadow(&I, getCleanShadow(&I));
     }
 
-    if (ClCheckAccessAddress)
-      insertShadowCheck(I.getPointerOperand(), &I);
+    maybeInsertAddressCheck(I.getPointerOperand(), &I);
 
     if (I.isAtomic())
       I.setOrdering(addAcquireOrdering(I.getOrdering()));
@@ -1432,10 +1673,9 @@
     IRBuilder<> IRB(&I);
     Value *Addr = I.getOperand(0);
     Value *ShadowPtr =
-        getShadowOriginPtr(Addr, IRB, I.getType(), /*Alignment*/ 1).first;
+        getShadowOriginPtr(Addr, IRB, I.getType(), /*Alignment*/ 1, /*isStore*/true).first;
 
-    if (ClCheckAccessAddress)
-      insertShadowCheck(Addr, &I);
+    maybeInsertAddressCheck(Addr, &I);
 
     // Only test the conditional argument of cmpxchg instruction.
     // The other argument can potentially be uninitialized, but we can not
@@ -2058,11 +2298,10 @@
     // We don't know the pointer alignment (could be unaligned SSE store!).
     // Have to assume to worst case.
     std::tie(ShadowPtr, OriginPtr) =
-        getShadowOriginPtr(Addr, IRB, Shadow->getType(), /*Alignment*/ 1);
+        getShadowOriginPtr(Addr, IRB, Shadow->getType(), /*Alignment*/ 1, /*isStore*/ true);
     IRB.CreateAlignedStore(Shadow, ShadowPtr, 1);
 
-    if (ClCheckAccessAddress)
-      insertShadowCheck(Addr, &I);
+    maybeInsertAddressCheck(Addr, &I);
 
     // FIXME: factor out common code from materializeStores
     if (MS.TrackOrigins) IRB.CreateStore(getOrigin(&I, 1), OriginPtr);
@@ -2084,14 +2323,13 @@
       // Have to assume to worst case.
       unsigned Alignment = 1;
       std::tie(ShadowPtr, OriginPtr) =
-          getShadowOriginPtr(Addr, IRB, ShadowTy, Alignment);
+          getShadowOriginPtr(Addr, IRB, ShadowTy, Alignment, /*isStore*/ false);
       setShadow(&I, IRB.CreateAlignedLoad(ShadowPtr, Alignment, "_msld"));
     } else {
       setShadow(&I, getCleanShadow(&I));
     }
 
-    if (ClCheckAccessAddress)
-      insertShadowCheck(Addr, &I);
+    maybeInsertAddressCheck(Addr, &I);
 
     if (MS.TrackOrigins) {
       if (PropagateShadow)
@@ -2453,13 +2691,12 @@
     IRBuilder<> IRB(&I);
     Value* Addr = I.getArgOperand(0);
     Type *Ty = IRB.getInt32Ty();
-    Value *ShadowPtr = getShadowOriginPtr(Addr, IRB, Ty, /*Alignment*/ 1).first;
+    Value *ShadowPtr = getShadowOriginPtr(Addr, IRB, Ty, /*Alignment*/ 1,  /*isStore*/ true).first;
 
     IRB.CreateStore(getCleanShadow(Ty),
                     IRB.CreatePointerCast(ShadowPtr, Ty->getPointerTo()));
 
-    if (ClCheckAccessAddress)
-      insertShadowCheck(Addr, &I);
+    maybeInsertAddressCheck(Addr, &I);
   }
 
   void handleLdmxcsr(IntrinsicInst &I) {
@@ -2470,11 +2707,10 @@
     Type *Ty = IRB.getInt32Ty();
     unsigned Alignment = 1;
     Value *ShadowPtr, *OriginPtr;
-    std::tie(ShadowPtr, OriginPtr) =
-        getShadowOriginPtr(Addr, IRB, Ty, Alignment);
+    std::tie(ShadowPtr, OriginPtr) = getShadowOriginPtr(
+        Addr, IRB, Ty, Alignment, /*isStore*/ false);
 
-    if (ClCheckAccessAddress)
-      insertShadowCheck(Addr, &I);
+    maybeInsertAddressCheck(Addr, &I);
 
     Value *Shadow = IRB.CreateAlignedLoad(ShadowPtr, Alignment, "_ldmxcsr");
     Value *Origin =
@@ -2766,9 +3002,10 @@
         unsigned ParamAlignment = CS.getParamAlignment(i);
         unsigned Alignment = std::min(ParamAlignment, kShadowTLSAlignment);
         Value *AShadowPtr =
-            getShadowOriginPtr(A, IRB, IRB.getInt8Ty(), Alignment).first;
+            getShadowOriginPtr(A, IRB, IRB.getInt8Ty(), Alignment, /*isStore*/false).first;
 
         Store = IRB.CreateMemCpy(ArgShadowBase, AShadowPtr, Size, Alignment);
+        // TODO(glider): what happens to origins here?
       } else {
         Size = DL.getTypeAllocSize(A->getType());
         if (ArgOffset + Size > kParamTLSSize) break;
@@ -2875,21 +3112,27 @@
                                   "_msphi_o"));
   }
 
-  void visitAllocaInst(AllocaInst &I) {
-    setShadow(&I, getCleanShadow(&I));
-    setOrigin(&I, getCleanOrigin());
-    IRBuilder<> IRB(I.getNextNode());
-    const DataLayout &DL = F.getParent()->getDataLayout();
-    uint64_t TypeSize = DL.getTypeAllocSize(I.getAllocatedType());
-    Value *Len = ConstantInt::get(MS.IntptrTy, TypeSize);
-    if (I.isArrayAllocation())
-      Len = IRB.CreateMul(Len, I.getArraySize());
+  Value *getLocalVarDescription(AllocaInst &I) {
+    SmallString<2048> StackDescriptionStorage;
+    raw_svector_ostream StackDescription(StackDescriptionStorage);
+    // We create a string with a description of the stack allocation and
+    // pass it into __msan_set_alloca_origin.
+    // It will be printed by the run-time if stack-originated UMR is found.
+    // The first 4 bytes of the string are set to '----' and will be replaced
+    // by __msan_va_arg_overflow_size_tls at the first call.
+    StackDescription << "----" << I.getName() << "@" << F.getName();
+    return createPrivateNonConstGlobalForString(*F.getParent(),
+                                                StackDescription.str());
+  }
+
+  void instrumentAllocaUserspace(AllocaInst &I, IRBuilder<> &IRB, Value *Len) {
     if (PoisonStack && ClPoisonStackWithCall) {
       IRB.CreateCall(MS.MsanPoisonStackFn,
                      {IRB.CreatePointerCast(&I, IRB.getInt8PtrTy()), Len});
     } else {
-      Value *ShadowBase =
-          getShadowOriginPtr(&I, IRB, IRB.getInt8Ty(), I.getAlignment()).first;
+      Value *ShadowBase, *OriginBase;
+      std::tie(ShadowBase, OriginBase) = getShadowOriginPtr(
+          &I, IRB, IRB.getInt8Ty(), 1, /*isStore*/ true);
 
       Value *PoisonValue = IRB.getInt8(PoisonStack ? ClPoisonStackPattern : 0);
       IRB.CreateMemSet(ShadowBase, PoisonValue, Len, I.getAlignment());
@@ -2896,18 +3139,7 @@
     }
 
     if (PoisonStack && MS.TrackOrigins) {
-      SmallString<2048> StackDescriptionStorage;
-      raw_svector_ostream StackDescription(StackDescriptionStorage);
-      // We create a string with a description of the stack allocation and
-      // pass it into __msan_set_alloca_origin.
-      // It will be printed by the run-time if stack-originated UMR is found.
-      // The first 4 bytes of the string are set to '----' and will be replaced
-      // by __msan_va_arg_overflow_size_tls at the first call.
-      StackDescription << "----" << I.getName() << "@" << F.getName();
-      Value *Descr =
-          createPrivateNonConstGlobalForString(*F.getParent(),
-                                               StackDescription.str());
-
+      Value *Descr = getLocalVarDescription(I);
       IRB.CreateCall(MS.MsanSetAllocaOrigin4Fn,
                      {IRB.CreatePointerCast(&I, IRB.getInt8PtrTy()), Len,
                       IRB.CreatePointerCast(Descr, IRB.getInt8PtrTy()),
@@ -2915,6 +3147,32 @@
     }
   }
 
+  void instrumentAllocaKmsan(AllocaInst &I, IRBuilder<> &IRB, Value *Len) {
+    Value *Descr = getLocalVarDescription(I);
+    Value *Pc = IRB.CreateCall(
+        Intrinsic::getDeclaration(F.getParent(), Intrinsic::returnaddress),
+        IRB.getInt32(0));
+    IRB.CreateCall(MS.MsanPoisonAllocaFn,
+                   {IRB.CreatePointerCast(&I, IRB.getInt8PtrTy()), Len,
+                    IRB.CreatePointerCast(Descr, IRB.getInt8PtrTy()),
+                    IRB.CreatePointerCast(Pc, MS.IntptrTy)});
+  }
+
+  void visitAllocaInst(AllocaInst &I) {
+    setShadow(&I, getCleanShadow(&I));
+    setOrigin(&I, getCleanOrigin());
+    IRBuilder<> IRB(I.getNextNode());
+    const DataLayout &DL = F.getParent()->getDataLayout();
+    uint64_t TypeSize = DL.getTypeAllocSize(I.getAllocatedType());
+    Value *Len = ConstantInt::get(MS.IntptrTy, TypeSize);
+    if (I.isArrayAllocation()) Len = IRB.CreateMul(Len, I.getArraySize());
+
+    if (!MS.CompileKernel)
+      instrumentAllocaUserspace(I, IRB, Len);
+    else
+      instrumentAllocaKmsan(I, IRB, Len);
+  }
+
   void visitSelectInst(SelectInst& I) {
     IRBuilder<> IRB(&I);
     // a = select b, c, d
@@ -3042,13 +3300,34 @@
     if (ClDumpStrictInstructions)
       dumpInst(I);
     DEBUG(dbgs() << "DEFAULT: " << I << "\n");
+#if 1
+    CallInst *Call = dyn_cast_or_null<CallInst>(&I);
+    bool is_asm = false;
+    if (Call && Call->isInlineAsm()) {
+      ///errs() << "visitInstruction: " << *Call << "\n";
+      ///is_asm = true;
+    }
+#endif
     for (size_t i = 0, n = I.getNumOperands(); i < n; i++) {
       Value *Operand = I.getOperand(i);
+      if (is_asm) {
+        errs() << "Operand #" << i << " " << *Operand << "\n";
+        errs() << "Type: " << *Operand->getType() << "\n";
+        errs() << "Shadow: " << *getShadow(Operand) << "\n";
+      }
       if (Operand->getType()->isSized())
         insertShadowCheck(Operand, &I);
     }
     setShadow(&I, getCleanShadow(&I));
     setOrigin(&I, getCleanOrigin());
+    if (is_asm) {
+      Value *Shadow = getShadow(&I);
+      if (Shadow)
+        errs() << "Shadow: " << *Shadow << "\n";
+      else
+        errs() << "Shadow: null\n";
+      errs() << "\n";
+    }
   }
 };
 
@@ -3063,6 +3342,7 @@
   MemorySanitizer &MS;
   MemorySanitizerVisitor &MSV;
   Value *VAArgTLSCopy = nullptr;
+  Value *VAArgTLSOriginCopy = nullptr;
   Value *VAArgOverflowSize = nullptr;
 
   SmallVector<CallInst*, 16> VAStartInstrumentationList;
@@ -3114,12 +3394,20 @@
         uint64_t ArgSize = DL.getTypeAllocSize(RealTy);
         Value *ShadowBase =
             getShadowPtrForVAArgument(RealTy, IRB, OverflowOffset);
+        Value *OriginBase = nullptr;
+        if (MS.CompileKernel) {
+          OriginBase = getOriginPtrForVAArgument(RealTy, IRB, OverflowOffset);
+        }
         OverflowOffset += alignTo(ArgSize, 8);
         Value *ShadowPtr, *OriginPtr;
-        std::tie(ShadowPtr, OriginPtr) = MSV.getShadowOriginPtr(
-            A, IRB, IRB.getInt8Ty(), kShadowTLSAlignment);
+        std::tie(ShadowPtr, OriginPtr) =
+            MSV.getShadowOriginPtr(A, IRB, IRB.getInt8Ty(), kShadowTLSAlignment,
+                                   /*isStore*/ false);
 
         IRB.CreateMemCpy(ShadowBase, ShadowPtr, ArgSize, kShadowTLSAlignment);
+        if (MS.CompileKernel) {
+          IRB.CreateMemCpy(OriginBase, OriginPtr, ArgSize, kShadowTLSAlignment);
+        }
       } else {
         ArgKind AK = classifyArgument(A);
         if (AK == AK_GeneralPurpose && GpOffset >= AMD64GpEndOffset)
@@ -3126,14 +3414,20 @@
           AK = AK_Memory;
         if (AK == AK_FloatingPoint && FpOffset >= AMD64FpEndOffset)
           AK = AK_Memory;
-        Value *ShadowBase;
+        Value *ShadowBase, *OriginBase;
         switch (AK) {
           case AK_GeneralPurpose:
             ShadowBase = getShadowPtrForVAArgument(A->getType(), IRB, GpOffset);
+            if (MS.CompileKernel)
+              OriginBase =
+                  getOriginPtrForVAArgument(A->getType(), IRB, GpOffset);
             GpOffset += 8;
             break;
           case AK_FloatingPoint:
             ShadowBase = getShadowPtrForVAArgument(A->getType(), IRB, FpOffset);
+            if (MS.CompileKernel)
+              OriginBase =
+                  getOriginPtrForVAArgument(A->getType(), IRB, FpOffset);
             FpOffset += 16;
             break;
           case AK_Memory:
@@ -3142,6 +3436,9 @@
             uint64_t ArgSize = DL.getTypeAllocSize(A->getType());
             ShadowBase =
                 getShadowPtrForVAArgument(A->getType(), IRB, OverflowOffset);
+            if (MS.CompileKernel)
+              OriginBase =
+                  getOriginPtrForVAArgument(A->getType(), IRB, OverflowOffset);
             OverflowOffset += alignTo(ArgSize, 8);
         }
         // Take fixed arguments into account for GpOffset and FpOffset,
@@ -3148,8 +3445,16 @@
         // but don't actually store shadows for them.
         if (IsFixed)
           continue;
-        IRB.CreateAlignedStore(MSV.getShadow(A), ShadowBase,
+        Value *Shadow = MSV.getShadow(A);
+        IRB.CreateAlignedStore(Shadow, ShadowBase,
                                kShadowTLSAlignment);
+        if (MS.CompileKernel) {
+          // TODO(glider): doesn't quite work. Need to fill region with origin.
+          unsigned StoreSize = DL.getTypeStoreSize(Shadow->getType());
+          MSV.paintOrigin(IRB, MSV.getOrigin(A), OriginBase, StoreSize, std::max(kShadowTLSAlignment, kMinOriginAlignment));
+          ///IRB.CreateAlignedStore(MSV.getOrigin(A), OriginBase,
+          ///                       kShadowTLSAlignment);
+        }
       }
     }
     Constant *OverflowSize =
@@ -3163,16 +3468,26 @@
     Value *Base = IRB.CreatePointerCast(MS.VAArgTLS, MS.IntptrTy);
     Base = IRB.CreateAdd(Base, ConstantInt::get(MS.IntptrTy, ArgOffset));
     return IRB.CreateIntToPtr(Base, PointerType::get(MSV.getShadowTy(Ty), 0),
-                              "_msarg");
+                              "_msarg_va_s");
   }
 
+  /// \brief Compute the shadow address for a given va_arg.
+  Value *getOriginPtrForVAArgument(Type *Ty, IRBuilder<> &IRB, int ArgOffset) {
+    Value *Base = IRB.CreatePointerCast(MS.VAArgOriginTLS, MS.IntptrTy);
+    Base = IRB.CreateAdd(Base, ConstantInt::get(MS.IntptrTy, ArgOffset));
+    return IRB.CreateIntToPtr(Base, PointerType::get(MSV.getShadowTy(Ty), 0),
+                              "_msarg_va_o");
+  }
+
   void unpoisonVAListTagForInst(IntrinsicInst &I) {
     IRBuilder<> IRB(&I);
     Value *VAListTag = I.getArgOperand(0);
+
     Value *ShadowPtr, *OriginPtr;
     unsigned Alignment = 8;
     std::tie(ShadowPtr, OriginPtr) =
-        MSV.getShadowOriginPtr(VAListTag, IRB, IRB.getInt8Ty(), Alignment);
+        MSV.getShadowOriginPtr(VAListTag, IRB, IRB.getInt8Ty(), Alignment,
+                               /*isStore*/ true);
 
     // Unpoison the whole __va_list_tag.
     // FIXME: magic ABI constants.
@@ -3200,7 +3515,7 @@
     if (!VAStartInstrumentationList.empty()) {
       // If there is a va_start in this function, make a backup copy of
       // va_arg_tls somewhere in the function entry block.
-      IRBuilder<> IRB(F.getEntryBlock().getFirstNonPHI());
+      IRBuilder<> IRB(MSV.ActualFnStart->getFirstNonPHI());
       VAArgOverflowSize = IRB.CreateLoad(MS.VAArgOverflowSizeTLS);
       Value *CopySize =
         IRB.CreateAdd(ConstantInt::get(MS.IntptrTy, AMD64FpEndOffset),
@@ -3207,6 +3522,10 @@
                       VAArgOverflowSize);
       VAArgTLSCopy = IRB.CreateAlloca(Type::getInt8Ty(*MS.C), CopySize);
       IRB.CreateMemCpy(VAArgTLSCopy, MS.VAArgTLS, CopySize, 8);
+      if (MS.CompileKernel) {
+        VAArgTLSOriginCopy = IRB.CreateAlloca(Type::getInt8Ty(*MS.C), CopySize);
+        IRB.CreateMemCpy(VAArgTLSOriginCopy, MS.VAArgOriginTLS, CopySize, 8);
+      }
     }
 
     // Instrument va_start.
@@ -3219,28 +3538,39 @@
       Value *RegSaveAreaPtrPtr = IRB.CreateIntToPtr(
           IRB.CreateAdd(IRB.CreatePtrToInt(VAListTag, MS.IntptrTy),
                         ConstantInt::get(MS.IntptrTy, 16)),
-          Type::getInt64PtrTy(*MS.C));
+          PointerType::get(Type::getInt64PtrTy(*MS.C), 0));
       Value *RegSaveAreaPtr = IRB.CreateLoad(RegSaveAreaPtrPtr);
       Value *RegSaveAreaShadowPtr, *RegSaveAreaOriginPtr;
       unsigned Alignment = 16;
       std::tie(RegSaveAreaShadowPtr, RegSaveAreaOriginPtr) =
           MSV.getShadowOriginPtr(RegSaveAreaPtr, IRB, IRB.getInt8Ty(),
-                                 Alignment);
+                                 Alignment, /*isStore*/ true);
       IRB.CreateMemCpy(RegSaveAreaShadowPtr, VAArgTLSCopy, AMD64FpEndOffset,
                        Alignment);
+      if (MS.CompileKernel) {
+        IRB.CreateMemCpy(RegSaveAreaOriginPtr, VAArgTLSOriginCopy,
+                         AMD64FpEndOffset, Alignment);
+      }
       Value *OverflowArgAreaPtrPtr = IRB.CreateIntToPtr(
           IRB.CreateAdd(IRB.CreatePtrToInt(VAListTag, MS.IntptrTy),
                         ConstantInt::get(MS.IntptrTy, 8)),
-          Type::getInt64PtrTy(*MS.C));
+          PointerType::get(Type::getInt64PtrTy(*MS.C), 0));
       Value *OverflowArgAreaPtr = IRB.CreateLoad(OverflowArgAreaPtrPtr);
+
       Value *OverflowArgAreaShadowPtr, *OverflowArgAreaOriginPtr;
       std::tie(OverflowArgAreaShadowPtr, OverflowArgAreaOriginPtr) =
           MSV.getShadowOriginPtr(OverflowArgAreaPtr, IRB, IRB.getInt8Ty(),
-                                 Alignment);
+                                 Alignment, /*isStore*/ true);
       Value *SrcPtr = IRB.CreateConstGEP1_32(IRB.getInt8Ty(), VAArgTLSCopy,
                                              AMD64FpEndOffset);
       IRB.CreateMemCpy(OverflowArgAreaShadowPtr, SrcPtr, VAArgOverflowSize,
                        Alignment);
+      if (MS.CompileKernel) {
+        SrcPtr = IRB.CreateConstGEP1_32(IRB.getInt8Ty(), VAArgTLSOriginCopy,
+                                        AMD64FpEndOffset);
+        IRB.CreateMemCpy(OverflowArgAreaOriginPtr, SrcPtr, VAArgOverflowSize,
+                         Alignment);
+      }
     }
   }
 };
@@ -3302,7 +3632,7 @@
     Value *ShadowPtr, *OriginPtr;
     unsigned Alignment = 8;
     std::tie(ShadowPtr, OriginPtr) =
-        MSV.getShadowOriginPtr(VAListTag, IRB, IRB.getInt8Ty(), Alignment);
+        MSV.getShadowOriginPtr(VAListTag, IRB, IRB.getInt8Ty(), Alignment, /*isStore*/ true);
     IRB.CreateMemSet(ShadowPtr, Constant::getNullValue(IRB.getInt8Ty()),
                      /* size */ 8, Alignment, false);
   }
@@ -3314,7 +3644,7 @@
     Value *ShadowPtr, *OriginPtr;
     unsigned Alignment = 8;
     std::tie(ShadowPtr, OriginPtr) =
-        MSV.getShadowOriginPtr(VAListTag, IRB, IRB.getInt8Ty(), Alignment);
+        MSV.getShadowOriginPtr(VAListTag, IRB, IRB.getInt8Ty(), Alignment, /*isStore*/ true);
     IRB.CreateMemSet(ShadowPtr, Constant::getNullValue(IRB.getInt8Ty()),
                      /* size */ 8, Alignment, false);
   }
@@ -3322,7 +3652,7 @@
   void finalizeInstrumentation() override {
     assert(!VAArgSize && !VAArgTLSCopy &&
            "finalizeInstrumentation called twice");
-    IRBuilder<> IRB(F.getEntryBlock().getFirstNonPHI());
+    IRBuilder<> IRB(MSV.ActualFnStart->getFirstNonPHI());
     VAArgSize = IRB.CreateLoad(MS.VAArgOverflowSizeTLS);
     Value *CopySize = IRB.CreateAdd(ConstantInt::get(MS.IntptrTy, 0),
                                     VAArgSize);
@@ -3341,14 +3671,14 @@
       IRBuilder<> IRB(OrigInst->getNextNode());
       Value *VAListTag = OrigInst->getArgOperand(0);
       Value *RegSaveAreaPtrPtr =
-        IRB.CreateIntToPtr(IRB.CreatePtrToInt(VAListTag, MS.IntptrTy),
-                        Type::getInt64PtrTy(*MS.C));
+          IRB.CreateIntToPtr(IRB.CreatePtrToInt(VAListTag, MS.IntptrTy),
+                             PointerType::get(Type::getInt64PtrTy(*MS.C), 0));
       Value *RegSaveAreaPtr = IRB.CreateLoad(RegSaveAreaPtrPtr);
       Value *RegSaveAreaShadowPtr, *RegSaveAreaOriginPtr;
       unsigned Alignment = 8;
       std::tie(RegSaveAreaShadowPtr, RegSaveAreaOriginPtr) =
           MSV.getShadowOriginPtr(RegSaveAreaPtr, IRB, IRB.getInt8Ty(),
-                                 Alignment);
+                                 Alignment, /*isStore*/ true);
       IRB.CreateMemCpy(RegSaveAreaShadowPtr, VAArgTLSCopy, CopySize, Alignment);
     }
   }
@@ -3462,7 +3792,7 @@
     Value *ShadowPtr, *OriginPtr;
     unsigned Alignment = 8;
     std::tie(ShadowPtr, OriginPtr) =
-        MSV.getShadowOriginPtr(VAListTag, IRB, IRB.getInt8Ty(), Alignment);
+        MSV.getShadowOriginPtr(VAListTag, IRB, IRB.getInt8Ty(), Alignment, /*isStore*/ true);
     IRB.CreateMemSet(ShadowPtr, Constant::getNullValue(IRB.getInt8Ty()),
                      /* size */ 32, Alignment, false);
   }
@@ -3474,7 +3804,7 @@
     Value *ShadowPtr, *OriginPtr;
     unsigned Alignment = 8;
     std::tie(ShadowPtr, OriginPtr) =
-        MSV.getShadowOriginPtr(VAListTag, IRB, IRB.getInt8Ty(), Alignment);
+        MSV.getShadowOriginPtr(VAListTag, IRB, IRB.getInt8Ty(), Alignment, /*isStore*/ true);
     IRB.CreateMemSet(ShadowPtr, Constant::getNullValue(IRB.getInt8Ty()),
                      /* size */ 32, Alignment, false);
   }
@@ -3506,7 +3836,7 @@
     if (!VAStartInstrumentationList.empty()) {
       // If there is a va_start in this function, make a backup copy of
       // va_arg_tls somewhere in the function entry block.
-      IRBuilder<> IRB(F.getEntryBlock().getFirstNonPHI());
+      IRBuilder<> IRB(MSV.ActualFnStart->getFirstNonPHI());
       VAArgOverflowSize = IRB.CreateLoad(MS.VAArgOverflowSizeTLS);
       Value *CopySize =
         IRB.CreateAdd(ConstantInt::get(MS.IntptrTy, AArch64VAEndOffset),
@@ -3563,7 +3893,7 @@
 
       Value *GrRegSaveAreaShadowPtr =
           MSV.getShadowOriginPtr(GrRegSaveAreaPtr, IRB, IRB.getInt8Ty(),
-                                 /*Alignment*/ 8)
+                                 /*Alignment*/ 8, /*isStore*/ true)
               .first;
 
       Value *GrSrcPtr = IRB.CreateInBoundsGEP(IRB.getInt8Ty(), VAArgTLSCopy,
@@ -3578,7 +3908,7 @@
 
       Value *VrRegSaveAreaShadowPtr =
           MSV.getShadowOriginPtr(VrRegSaveAreaPtr, IRB, IRB.getInt8Ty(),
-                                 /*Alignment*/ 8)
+                                 /*Alignment*/ 8, /*isStore*/ true)
               .first;
 
       Value *VrSrcPtr = IRB.CreateInBoundsGEP(
@@ -3593,7 +3923,7 @@
       // And finally for remaining arguments.
       Value *StackSaveAreaShadowPtr =
           MSV.getShadowOriginPtr(StackSaveAreaPtr, IRB, IRB.getInt8Ty(),
-                                 /*Alignment*/ 16)
+                                 /*Alignment*/ 16, /*isStore*/ true)
               .first;
 
       Value *StackSrcPtr =
@@ -3657,9 +3987,10 @@
                                                   VAArgOffset - VAArgBase);
           Value *AShadowPtr, *AOriginPtr;
           std::tie(AShadowPtr, AOriginPtr) = MSV.getShadowOriginPtr(
-              A, IRB, IRB.getInt8Ty(), kShadowTLSAlignment);
+              A, IRB, IRB.getInt8Ty(), kShadowTLSAlignment, /*isStore*/ false);
 
           IRB.CreateMemCpy(Base, AShadowPtr, ArgSize, kShadowTLSAlignment);
+          // TODO(glider): origins?
         }
         VAArgOffset += alignTo(ArgSize, 8);
       } else {
@@ -3710,7 +4041,7 @@
     Value *Base = IRB.CreatePointerCast(MS.VAArgTLS, MS.IntptrTy);
     Base = IRB.CreateAdd(Base, ConstantInt::get(MS.IntptrTy, ArgOffset));
     return IRB.CreateIntToPtr(Base, PointerType::get(MSV.getShadowTy(Ty), 0),
-                              "_msarg");
+                              "_msarg_s");
   }
 
   void visitVAStartInst(VAStartInst &I) override {
@@ -3720,7 +4051,7 @@
     Value *ShadowPtr, *OriginPtr;
     unsigned Alignment = 8;
     std::tie(ShadowPtr, OriginPtr) =
-        MSV.getShadowOriginPtr(VAListTag, IRB, IRB.getInt8Ty(), Alignment);
+        MSV.getShadowOriginPtr(VAListTag, IRB, IRB.getInt8Ty(), Alignment, /*isStore*/ true);
     IRB.CreateMemSet(ShadowPtr, Constant::getNullValue(IRB.getInt8Ty()),
                      /* size */ 8, Alignment, false);
   }
@@ -3731,7 +4062,7 @@
     Value *ShadowPtr, *OriginPtr;
     unsigned Alignment = 8;
     std::tie(ShadowPtr, OriginPtr) =
-        MSV.getShadowOriginPtr(VAListTag, IRB, IRB.getInt8Ty(), Alignment);
+        MSV.getShadowOriginPtr(VAListTag, IRB, IRB.getInt8Ty(), Alignment, /*isStore*/ true);
     // Unpoison the whole __va_list_tag.
     // FIXME: magic ABI constants.
     IRB.CreateMemSet(ShadowPtr, Constant::getNullValue(IRB.getInt8Ty()),
@@ -3741,7 +4072,7 @@
   void finalizeInstrumentation() override {
     assert(!VAArgSize && !VAArgTLSCopy &&
            "finalizeInstrumentation called twice");
-    IRBuilder<> IRB(F.getEntryBlock().getFirstNonPHI());
+    IRBuilder<> IRB(MSV.ActualFnStart->getFirstNonPHI());
     VAArgSize = IRB.CreateLoad(MS.VAArgOverflowSizeTLS);
     Value *CopySize = IRB.CreateAdd(ConstantInt::get(MS.IntptrTy, 0),
                                     VAArgSize);
@@ -3760,14 +4091,14 @@
       IRBuilder<> IRB(OrigInst->getNextNode());
       Value *VAListTag = OrigInst->getArgOperand(0);
       Value *RegSaveAreaPtrPtr =
-        IRB.CreateIntToPtr(IRB.CreatePtrToInt(VAListTag, MS.IntptrTy),
-                        Type::getInt64PtrTy(*MS.C));
+          IRB.CreateIntToPtr(IRB.CreatePtrToInt(VAListTag, MS.IntptrTy),
+                             PointerType::get(Type::getInt64PtrTy(*MS.C), 0));
       Value *RegSaveAreaPtr = IRB.CreateLoad(RegSaveAreaPtrPtr);
       Value *RegSaveAreaShadowPtr, *RegSaveAreaOriginPtr;
       unsigned Alignment = 8;
       std::tie(RegSaveAreaShadowPtr, RegSaveAreaOriginPtr) =
           MSV.getShadowOriginPtr(RegSaveAreaPtr, IRB, IRB.getInt8Ty(),
-                                 Alignment);
+                                 Alignment, /*isStore*/ true);
       IRB.CreateMemCpy(RegSaveAreaShadowPtr, VAArgTLSCopy, CopySize, Alignment);
     }
   }
Index: test/Instrumentation/MemorySanitizer/alloca.ll
===================================================================
--- test/Instrumentation/MemorySanitizer/alloca.ll	(revision 320373)
+++ test/Instrumentation/MemorySanitizer/alloca.ll	(working copy)
@@ -2,6 +2,7 @@
 ; RUN: opt < %s -msan -msan-check-access-address=0 -msan-poison-stack-with-call=1 -S | FileCheck %s --check-prefixes=CHECK,CALL
 ; RUN: opt < %s -msan -msan-check-access-address=0 -msan-track-origins=1 -S | FileCheck %s --check-prefixes=CHECK,ORIGIN
 ; RUN: opt < %s -msan -msan-check-access-address=0 -msan-track-origins=2 -S | FileCheck %s --check-prefixes=CHECK,ORIGIN
+; RUN: opt < %s -msan -msan-kernel=1 -S | FileCheck %s --check-prefixes=CHECK,KMSAN
 
 target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
 target triple = "x86_64-unknown-linux-gnu"
@@ -16,6 +17,7 @@
 ; INLINE: call void @llvm.memset.p0i8.i64(i8* {{.*}}, i8 -1, i64 4, i32 4, i1 false)
 ; CALL: call void @__msan_poison_stack(i8* {{.*}}, i64 4)
 ; ORIGIN: call void @__msan_set_alloca_origin4(i8* {{.*}}, i64 4,
+; KMSAN: call void @__msan_poison_alloca(i8* {{.*}}, i64 4,
 ; CHECK: ret void
 
 
@@ -31,6 +33,7 @@
 ; INLINE: call void @llvm.memset.p0i8.i64(i8* {{.*}}, i8 -1, i64 4, i32 4, i1 false)
 ; CALL: call void @__msan_poison_stack(i8* {{.*}}, i64 4)
 ; ORIGIN: call void @__msan_set_alloca_origin4(i8* {{.*}}, i64 4,
+; KMSAN: call void @__msan_poison_alloca(i8* {{.*}}, i64 4,
 ; CHECK: ret void
 
 define void @array() sanitize_memory {
@@ -43,6 +46,7 @@
 ; INLINE: call void @llvm.memset.p0i8.i64(i8* {{.*}}, i8 -1, i64 20, i32 4, i1 false)
 ; CALL: call void @__msan_poison_stack(i8* {{.*}}, i64 20)
 ; ORIGIN: call void @__msan_set_alloca_origin4(i8* {{.*}}, i64 20,
+; KMSAN: call void @__msan_poison_alloca(i8* {{.*}}, i64 20,
 ; CHECK: ret void
 
 define void @array_non_const(i64 %cnt) sanitize_memory {
@@ -56,4 +60,5 @@
 ; INLINE: call void @llvm.memset.p0i8.i64(i8* {{.*}}, i8 -1, i64 %[[A]], i32 4, i1 false)
 ; CALL: call void @__msan_poison_stack(i8* {{.*}}, i64 %[[A]])
 ; ORIGIN: call void @__msan_set_alloca_origin4(i8* {{.*}}, i64 %[[A]],
+; KMSAN: call void @__msan_poison_alloca(i8* {{.*}}, i64 %[[A]],
 ; CHECK: ret void
Index: test/Instrumentation/MemorySanitizer/atomics.ll
===================================================================
--- test/Instrumentation/MemorySanitizer/atomics.ll	(revision 320373)
+++ test/Instrumentation/MemorySanitizer/atomics.ll	(working copy)
@@ -1,6 +1,7 @@
-; RUN: opt < %s -msan -msan-check-access-address=0 -S | FileCheck %s
-; RUN: opt < %s -msan -msan-check-access-address=0 -msan-track-origins=1 -S | FileCheck %s
-; RUN: opt < %s -msan -msan-check-access-address=0 -msan-track-origins=2 -S | FileCheck %s
+; RUN: opt < %s -msan -msan-check-access-address=0 -S | FileCheck %s -check-prefix=CHECK -check-prefix=CHECK-MSAN
+; RUN: opt < %s -msan -msan-check-access-address=0 -msan-track-origins=1 -S | FileCheck %s -check-prefix=CHECK -check-prefix=CHECK-MSAN
+; RUN: opt < %s -msan -msan-check-access-address=0 -msan-track-origins=2 -S | FileCheck %s -check-prefix=CHECK -check-prefix=CHECK-MSAN
+; RUN: opt < %s -msan -msan-kernel -S | FileCheck %s -check-prefix=CHECK -check-prefix=CHECK-KMSAN
 
 target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
 target triple = "x86_64-unknown-linux-gnu"
@@ -16,7 +17,9 @@
 ; CHECK-LABEL: @AtomicRmwXchg
 ; CHECK: store i32 0,
 ; CHECK: atomicrmw xchg {{.*}} seq_cst
-; CHECK: store i32 0, {{.*}} @__msan_retval_tls
+; CHECK-MSAN: store i32 0, {{.*}} @__msan_retval_tls
+; CHECK-KMSAN: %retval_shadow
+; CHECK-KMSAN: store i32 0
 ; CHECK: ret i32
 
 
@@ -31,7 +34,9 @@
 ; CHECK-LABEL: @AtomicRmwMax
 ; CHECK: store i32 0,
 ; CHECK: atomicrmw max {{.*}} seq_cst
-; CHECK: store i32 0, {{.*}} @__msan_retval_tls
+; CHECK-MSAN: store i32 0, {{.*}} @__msan_retval_tls
+; CHECK-KMSAN: %retval_shadow
+; CHECK-KMSAN: store i32 0
 ; CHECK: ret i32
 
 
@@ -50,7 +55,9 @@
 ; CHECK: br
 ; CHECK: @__msan_warning
 ; CHECK: cmpxchg {{.*}} seq_cst seq_cst
-; CHECK: store i32 0, {{.*}} @__msan_retval_tls
+; CHECK-MSAN: store i32 0, {{.*}} @__msan_retval_tls
+; CHECK-KMSAN: %retval_shadow
+; CHECK-KMSAN: store i32 0
 ; CHECK: ret i32
 
 
@@ -69,7 +76,9 @@
 ; CHECK: br
 ; CHECK: @__msan_warning
 ; CHECK: cmpxchg {{.*}} release monotonic
-; CHECK: store i32 0, {{.*}} @__msan_retval_tls
+; CHECK-MSAN: store i32 0, {{.*}} @__msan_retval_tls
+; CHECK-KMSAN: %retval_shadow
+; CHECK-KMSAN: store i32 0
 ; CHECK: ret i32
 
 
@@ -84,7 +93,9 @@
 ; CHECK-LABEL: @AtomicLoad
 ; CHECK: load atomic i32, i32* {{.*}} seq_cst, align 16
 ; CHECK: [[SHADOW:%[01-9a-z_]+]] = load i32, i32* {{.*}}, align 16
-; CHECK: store i32 {{.*}}[[SHADOW]], {{.*}} @__msan_retval_tls
+; CHECK-MSAN: store i32 {{.*}}[[SHADOW]], {{.*}} @__msan_retval_tls
+; CHECK-KMSAN: %retval_shadow
+; CHECK-KMSAN: store i32 {{.*}}[[SHADOW]]
 ; CHECK: ret i32
 
 
@@ -99,7 +110,9 @@
 ; CHECK-LABEL: @AtomicLoadAcquire
 ; CHECK: load atomic i32, i32* {{.*}} acquire, align 16
 ; CHECK: [[SHADOW:%[01-9a-z_]+]] = load i32, i32* {{.*}}, align 16
-; CHECK: store i32 {{.*}}[[SHADOW]], {{.*}} @__msan_retval_tls
+; CHECK-MSAN: store i32 {{.*}}[[SHADOW]], {{.*}} @__msan_retval_tls
+; CHECK-KMSAN: %retval_shadow
+; CHECK-KMSAN: store i32 {{.*}}[[SHADOW]]
 ; CHECK: ret i32
 
 
@@ -114,7 +127,9 @@
 ; CHECK-LABEL: @AtomicLoadMonotonic
 ; CHECK: load atomic i32, i32* {{.*}} acquire, align 16
 ; CHECK: [[SHADOW:%[01-9a-z_]+]] = load i32, i32* {{.*}}, align 16
-; CHECK: store i32 {{.*}}[[SHADOW]], {{.*}} @__msan_retval_tls
+; CHECK-MSAN: store i32 {{.*}}[[SHADOW]], {{.*}} @__msan_retval_tls
+; CHECK-KMSAN: %retval_shadow
+; CHECK-KMSAN: store i32 {{.*}}[[SHADOW]]
 ; CHECK: ret i32
 
 
@@ -129,7 +144,9 @@
 ; CHECK-LABEL: @AtomicLoadUnordered
 ; CHECK: load atomic i32, i32* {{.*}} acquire, align 16
 ; CHECK: [[SHADOW:%[01-9a-z_]+]] = load i32, i32* {{.*}}, align 16
-; CHECK: store i32 {{.*}}[[SHADOW]], {{.*}} @__msan_retval_tls
+; CHECK-MSAN: store i32 {{.*}}[[SHADOW]], {{.*}} @__msan_retval_tls
+; CHECK-KMSAN: %retval_shadow
+; CHECK-KMSAN: store i32 {{.*}}[[SHADOW]]
 ; CHECK: ret i32
 
 
Index: test/Instrumentation/MemorySanitizer/byval-alignment.ll
===================================================================
--- test/Instrumentation/MemorySanitizer/byval-alignment.ll	(revision 320373)
+++ test/Instrumentation/MemorySanitizer/byval-alignment.ll	(working copy)
@@ -1,6 +1,7 @@
 ; Test that copy alignment for byval arguments is limited by param-tls slot alignment.
 
 ; RUN: opt < %s -msan -msan-check-access-address=0 -S | FileCheck %s
+; RUN: opt < %s -msan -msan-kernel=1 -S | FileCheck %s
 
 target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"
 target triple = "x86_64-unknown-linux-gnu"
Index: test/Instrumentation/MemorySanitizer/check_access_address.ll
===================================================================
--- test/Instrumentation/MemorySanitizer/check_access_address.ll	(revision 320373)
+++ test/Instrumentation/MemorySanitizer/check_access_address.ll	(working copy)
@@ -1,4 +1,5 @@
 ; RUN: opt < %s -msan -msan-check-access-address=1 -S | FileCheck %s
+; RUN: opt < %s -msan -msan-kernel -S | FileCheck %s
 
 target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
 target triple = "x86_64-unknown-linux-gnu"
Index: test/Instrumentation/MemorySanitizer/csr.ll
===================================================================
--- test/Instrumentation/MemorySanitizer/csr.ll	(revision 320373)
+++ test/Instrumentation/MemorySanitizer/csr.ll	(working copy)
@@ -1,5 +1,6 @@
 ; RUN: opt < %s -msan -msan-check-access-address=0 -S | FileCheck %s
-; RUN: opt < %s -msan -msan-check-access-address=1 -S | FileCheck %s --check-prefix=ADDR
+; RUN: opt < %s -msan -msan-check-access-address=1 -S | FileCheck %s --check-prefixes=CHECK,MSAN,ADDR
+; RUN: opt < %s -msan -msan-kernel=1 -S | FileCheck %s --check-prefixes=CHECK,KMSAN,ADDR
 ; REQUIRES: x86-registered-target
 
 target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
Index: test/Instrumentation/MemorySanitizer/msan_basic.ll
===================================================================
--- test/Instrumentation/MemorySanitizer/msan_basic.ll	(revision 320373)
+++ test/Instrumentation/MemorySanitizer/msan_basic.ll	(working copy)
@@ -1,955 +0,0 @@
-; RUN: opt < %s -msan -msan-check-access-address=0 -S | FileCheck %s
-; RUN: opt < %s -msan -msan-check-access-address=0 -msan-track-origins=1 -S | FileCheck -check-prefix=CHECK -check-prefix=CHECK-ORIGINS %s
-
-target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
-target triple = "x86_64-unknown-linux-gnu"
-
-; CHECK: @llvm.global_ctors {{.*}} { i32 0, void ()* @msan.module_ctor, i8* null }
-
-; Check the presence and the linkage type of __msan_track_origins and
-; other interface symbols.
-; CHECK-NOT: @__msan_track_origins
-; CHECK-ORIGINS: @__msan_track_origins = weak_odr constant i32 1
-; CHECK-NOT: @__msan_keep_going = weak_odr constant i32 0
-; CHECK: @__msan_retval_tls = external thread_local(initialexec) global [{{.*}}]
-; CHECK: @__msan_retval_origin_tls = external thread_local(initialexec) global i32
-; CHECK: @__msan_param_tls = external thread_local(initialexec) global [{{.*}}]
-; CHECK: @__msan_param_origin_tls = external thread_local(initialexec) global [{{.*}}]
-; CHECK: @__msan_va_arg_tls = external thread_local(initialexec) global [{{.*}}]
-; CHECK: @__msan_va_arg_overflow_size_tls = external thread_local(initialexec) global i64
-; CHECK: @__msan_origin_tls = external thread_local(initialexec) global i32
-
-
-; Check instrumentation of stores
-
-define void @Store(i32* nocapture %p, i32 %x) nounwind uwtable sanitize_memory {
-entry:
-  store i32 %x, i32* %p, align 4
-  ret void
-}
-
-; CHECK-LABEL: @Store
-; CHECK: load {{.*}} @__msan_param_tls
-; CHECK-ORIGINS: load {{.*}} @__msan_param_origin_tls
-; CHECK: store
-; CHECK-ORIGINS: icmp
-; CHECK-ORIGINS: br i1
-; CHECK-ORIGINS: <label>
-; CHECK-ORIGINS: store
-; CHECK-ORIGINS: br label
-; CHECK-ORIGINS: <label>
-; CHECK: store
-; CHECK: ret void
-
-
-; Check instrumentation of aligned stores
-; Shadow store has the same alignment as the original store; origin store
-; does not specify explicit alignment.
-
-define void @AlignedStore(i32* nocapture %p, i32 %x) nounwind uwtable sanitize_memory {
-entry:
-  store i32 %x, i32* %p, align 32
-  ret void
-}
-
-; CHECK-LABEL: @AlignedStore
-; CHECK: load {{.*}} @__msan_param_tls
-; CHECK-ORIGINS: load {{.*}} @__msan_param_origin_tls
-; CHECK: store {{.*}} align 32
-; CHECK-ORIGINS: icmp
-; CHECK-ORIGINS: br i1
-; CHECK-ORIGINS: <label>
-; CHECK-ORIGINS: store {{.*}} align 32
-; CHECK-ORIGINS: br label
-; CHECK-ORIGINS: <label>
-; CHECK: store {{.*}} align 32
-; CHECK: ret void
-
-
-; load followed by cmp: check that we load the shadow and call __msan_warning.
-define void @LoadAndCmp(i32* nocapture %a) nounwind uwtable sanitize_memory {
-entry:
-  %0 = load i32, i32* %a, align 4
-  %tobool = icmp eq i32 %0, 0
-  br i1 %tobool, label %if.end, label %if.then
-
-if.then:                                          ; preds = %entry
-  tail call void (...) @foo() nounwind
-  br label %if.end
-
-if.end:                                           ; preds = %entry, %if.then
-  ret void
-}
-
-declare void @foo(...)
-
-; CHECK-LABEL: @LoadAndCmp
-; CHECK: = load
-; CHECK: = load
-; CHECK: call void @__msan_warning_noreturn()
-; CHECK-NEXT: call void asm sideeffect
-; CHECK-NEXT: unreachable
-; CHECK: ret void
-
-; Check that we store the shadow for the retval.
-define i32 @ReturnInt() nounwind uwtable readnone sanitize_memory {
-entry:
-  ret i32 123
-}
-
-; CHECK-LABEL: @ReturnInt
-; CHECK: store i32 0,{{.*}}__msan_retval_tls
-; CHECK: ret i32
-
-; Check that we get the shadow for the retval.
-define void @CopyRetVal(i32* nocapture %a) nounwind uwtable sanitize_memory {
-entry:
-  %call = tail call i32 @ReturnInt() nounwind
-  store i32 %call, i32* %a, align 4
-  ret void
-}
-
-; CHECK-LABEL: @CopyRetVal
-; CHECK: load{{.*}}__msan_retval_tls
-; CHECK: store
-; CHECK: store
-; CHECK: ret void
-
-
-; Check that we generate PHIs for shadow.
-define void @FuncWithPhi(i32* nocapture %a, i32* %b, i32* nocapture %c) nounwind uwtable sanitize_memory {
-entry:
-  %tobool = icmp eq i32* %b, null
-  br i1 %tobool, label %if.else, label %if.then
-
-  if.then:                                          ; preds = %entry
-  %0 = load i32, i32* %b, align 4
-  br label %if.end
-
-  if.else:                                          ; preds = %entry
-  %1 = load i32, i32* %c, align 4
-  br label %if.end
-
-  if.end:                                           ; preds = %if.else, %if.then
-  %t.0 = phi i32 [ %0, %if.then ], [ %1, %if.else ]
-  store i32 %t.0, i32* %a, align 4
-  ret void
-}
-
-; CHECK-LABEL: @FuncWithPhi
-; CHECK: = phi
-; CHECK-NEXT: = phi
-; CHECK: store
-; CHECK: store
-; CHECK: ret void
-
-; Compute shadow for "x << 10"
-define void @ShlConst(i32* nocapture %x) nounwind uwtable sanitize_memory {
-entry:
-  %0 = load i32, i32* %x, align 4
-  %1 = shl i32 %0, 10
-  store i32 %1, i32* %x, align 4
-  ret void
-}
-
-; CHECK-LABEL: @ShlConst
-; CHECK: = load
-; CHECK: = load
-; CHECK: shl
-; CHECK: shl
-; CHECK: store
-; CHECK: store
-; CHECK: ret void
-
-; Compute shadow for "10 << x": it should have 'sext i1'.
-define void @ShlNonConst(i32* nocapture %x) nounwind uwtable sanitize_memory {
-entry:
-  %0 = load i32, i32* %x, align 4
-  %1 = shl i32 10, %0
-  store i32 %1, i32* %x, align 4
-  ret void
-}
-
-; CHECK-LABEL: @ShlNonConst
-; CHECK: = load
-; CHECK: = load
-; CHECK: = sext i1
-; CHECK: store
-; CHECK: store
-; CHECK: ret void
-
-; SExt
-define void @SExt(i32* nocapture %a, i16* nocapture %b) nounwind uwtable sanitize_memory {
-entry:
-  %0 = load i16, i16* %b, align 2
-  %1 = sext i16 %0 to i32
-  store i32 %1, i32* %a, align 4
-  ret void
-}
-
-; CHECK-LABEL: @SExt
-; CHECK: = load
-; CHECK: = load
-; CHECK: = sext
-; CHECK: = sext
-; CHECK: store
-; CHECK: store
-; CHECK: ret void
-
-
-; memset
-define void @MemSet(i8* nocapture %x) nounwind uwtable sanitize_memory {
-entry:
-  call void @llvm.memset.p0i8.i64(i8* %x, i8 42, i64 10, i32 1, i1 false)
-  ret void
-}
-
-declare void @llvm.memset.p0i8.i64(i8* nocapture, i8, i64, i32, i1) nounwind
-
-; CHECK-LABEL: @MemSet
-; CHECK: call i8* @__msan_memset
-; CHECK: ret void
-
-
-; memcpy
-define void @MemCpy(i8* nocapture %x, i8* nocapture %y) nounwind uwtable sanitize_memory {
-entry:
-  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %x, i8* %y, i64 10, i32 1, i1 false)
-  ret void
-}
-
-declare void @llvm.memcpy.p0i8.p0i8.i64(i8* nocapture, i8* nocapture, i64, i32, i1) nounwind
-
-; CHECK-LABEL: @MemCpy
-; CHECK: call i8* @__msan_memcpy
-; CHECK: ret void
-
-
-; memmove is lowered to a call
-define void @MemMove(i8* nocapture %x, i8* nocapture %y) nounwind uwtable sanitize_memory {
-entry:
-  call void @llvm.memmove.p0i8.p0i8.i64(i8* %x, i8* %y, i64 10, i32 1, i1 false)
-  ret void
-}
-
-declare void @llvm.memmove.p0i8.p0i8.i64(i8* nocapture, i8* nocapture, i64, i32, i1) nounwind
-
-; CHECK-LABEL: @MemMove
-; CHECK: call i8* @__msan_memmove
-; CHECK: ret void
-
-;; ------------
-;; Placeholder tests that will fail once element atomic @llvm.mem[cpy|move|set] instrinsics have
-;; been added to the MemIntrinsic class hierarchy. These will act as a reminder to
-;; verify that MSAN handles these intrinsics properly once they have been
-;; added to that class hierarchy.
-declare void @llvm.memset.element.unordered.atomic.p0i8.i64(i8* nocapture writeonly, i8, i64, i32) nounwind
-declare void @llvm.memmove.element.unordered.atomic.p0i8.p0i8.i64(i8* nocapture writeonly, i8* nocapture readonly, i64, i32) nounwind
-declare void @llvm.memcpy.element.unordered.atomic.p0i8.p0i8.i64(i8* nocapture writeonly, i8* nocapture readonly, i64, i32) nounwind
-
-define void @atomic_memcpy(i8* nocapture %x, i8* nocapture %y) nounwind {
-  ; CHECK-LABEL: atomic_memcpy
-  ; CHECK-NEXT: call void @llvm.memcpy.element.unordered.atomic.p0i8.p0i8.i64(i8* align 1 %x, i8* align 2 %y, i64 16, i32 1)
-  ; CHECK-NEXT: ret void
-  call void @llvm.memcpy.element.unordered.atomic.p0i8.p0i8.i64(i8* align 1 %x, i8* align 2 %y, i64 16, i32 1)
-  ret void
-}
-
-define void @atomic_memmove(i8* nocapture %x, i8* nocapture %y) nounwind {
-  ; CHECK-LABEL: atomic_memmove
-  ; CHECK-NEXT: call void @llvm.memmove.element.unordered.atomic.p0i8.p0i8.i64(i8* align 1 %x, i8* align 2 %y, i64 16, i32 1)
-  ; CHECK-NEXT: ret void
-  call void @llvm.memmove.element.unordered.atomic.p0i8.p0i8.i64(i8* align 1 %x, i8* align 2 %y, i64 16, i32 1)
-  ret void
-}
-
-define void @atomic_memset(i8* nocapture %x) nounwind {
-  ; CHECK-LABEL: atomic_memset
-  ; CHECK-NEXT: call void @llvm.memset.element.unordered.atomic.p0i8.i64(i8* align 1 %x, i8 88, i64 16, i32 1)
-  ; CHECK-NEXT: ret void
-  call void @llvm.memset.element.unordered.atomic.p0i8.i64(i8* align 1 %x, i8 88, i64 16, i32 1)
-  ret void
-}
-
-;; ------------
-
-
-; Check that we propagate shadow for "select"
-
-define i32 @Select(i32 %a, i32 %b, i1 %c) nounwind uwtable readnone sanitize_memory {
-entry:
-  %cond = select i1 %c, i32 %a, i32 %b
-  ret i32 %cond
-}
-
-; CHECK-LABEL: @Select
-; CHECK: select i1
-; CHECK-DAG: or i32
-; CHECK-DAG: xor i32
-; CHECK: or i32
-; CHECK-DAG: select i1
-; CHECK-ORIGINS-DAG: select
-; CHECK-ORIGINS-DAG: select
-; CHECK-DAG: select i1
-; CHECK: store i32{{.*}}@__msan_retval_tls
-; CHECK-ORIGINS: store i32{{.*}}@__msan_retval_origin_tls
-; CHECK: ret i32
-
-
-; Check that we propagate origin for "select" with vector condition.
-; Select condition is flattened to i1, which is then used to select one of the
-; argument origins.
-
-define <8 x i16> @SelectVector(<8 x i16> %a, <8 x i16> %b, <8 x i1> %c) nounwind uwtable readnone sanitize_memory {
-entry:
-  %cond = select <8 x i1> %c, <8 x i16> %a, <8 x i16> %b
-  ret <8 x i16> %cond
-}
-
-; CHECK-LABEL: @SelectVector
-; CHECK: select <8 x i1>
-; CHECK-DAG: or <8 x i16>
-; CHECK-DAG: xor <8 x i16>
-; CHECK: or <8 x i16>
-; CHECK-DAG: select <8 x i1>
-; CHECK-ORIGINS-DAG: select
-; CHECK-ORIGINS-DAG: select
-; CHECK-DAG: select <8 x i1>
-; CHECK: store <8 x i16>{{.*}}@__msan_retval_tls
-; CHECK-ORIGINS: store i32{{.*}}@__msan_retval_origin_tls
-; CHECK: ret <8 x i16>
-
-
-; Check that we propagate origin for "select" with scalar condition and vector
-; arguments. Select condition shadow is sign-extended to the vector type and
-; mixed into the result shadow.
-
-define <8 x i16> @SelectVector2(<8 x i16> %a, <8 x i16> %b, i1 %c) nounwind uwtable readnone sanitize_memory {
-entry:
-  %cond = select i1 %c, <8 x i16> %a, <8 x i16> %b
-  ret <8 x i16> %cond
-}
-
-; CHECK-LABEL: @SelectVector2
-; CHECK: select i1
-; CHECK-DAG: or <8 x i16>
-; CHECK-DAG: xor <8 x i16>
-; CHECK: or <8 x i16>
-; CHECK-DAG: select i1
-; CHECK-ORIGINS-DAG: select i1
-; CHECK-ORIGINS-DAG: select i1
-; CHECK-DAG: select i1
-; CHECK: ret <8 x i16>
-
-
-define { i64, i64 } @SelectStruct(i1 zeroext %x, { i64, i64 } %a, { i64, i64 } %b) readnone sanitize_memory {
-entry:
-  %c = select i1 %x, { i64, i64 } %a, { i64, i64 } %b
-  ret { i64, i64 } %c
-}
-
-; CHECK-LABEL: @SelectStruct
-; CHECK: select i1 {{.*}}, { i64, i64 }
-; CHECK-NEXT: select i1 {{.*}}, { i64, i64 } { i64 -1, i64 -1 }, { i64, i64 }
-; CHECK-ORIGINS: select i1
-; CHECK-ORIGINS: select i1
-; CHECK-NEXT: select i1 {{.*}}, { i64, i64 }
-; CHECK: ret { i64, i64 }
-
-
-define { i64*, double } @SelectStruct2(i1 zeroext %x, { i64*, double } %a, { i64*, double } %b) readnone sanitize_memory {
-entry:
-  %c = select i1 %x, { i64*, double } %a, { i64*, double } %b
-  ret { i64*, double } %c
-}
-
-; CHECK-LABEL: @SelectStruct2
-; CHECK: select i1 {{.*}}, { i64, i64 }
-; CHECK-NEXT: select i1 {{.*}}, { i64, i64 } { i64 -1, i64 -1 }, { i64, i64 }
-; CHECK-ORIGINS: select i1
-; CHECK-ORIGINS: select i1
-; CHECK-NEXT: select i1 {{.*}}, { i64*, double }
-; CHECK: ret { i64*, double }
-
-
-define i8* @IntToPtr(i64 %x) nounwind uwtable readnone sanitize_memory {
-entry:
-  %0 = inttoptr i64 %x to i8*
-  ret i8* %0
-}
-
-; CHECK-LABEL: @IntToPtr
-; CHECK: load i64, i64*{{.*}}__msan_param_tls
-; CHECK-ORIGINS-NEXT: load i32, i32*{{.*}}__msan_param_origin_tls
-; CHECK-NEXT: inttoptr
-; CHECK-NEXT: store i64{{.*}}__msan_retval_tls
-; CHECK: ret i8*
-
-
-define i8* @IntToPtr_ZExt(i16 %x) nounwind uwtable readnone sanitize_memory {
-entry:
-  %0 = inttoptr i16 %x to i8*
-  ret i8* %0
-}
-
-; CHECK-LABEL: @IntToPtr_ZExt
-; CHECK: load i16, i16*{{.*}}__msan_param_tls
-; CHECK: zext
-; CHECK-NEXT: inttoptr
-; CHECK-NEXT: store i64{{.*}}__msan_retval_tls
-; CHECK: ret i8*
-
-
-; Check that we insert exactly one check on udiv
-; (2nd arg shadow is checked, 1st arg shadow is propagated)
-
-define i32 @Div(i32 %a, i32 %b) nounwind uwtable readnone sanitize_memory {
-entry:
-  %div = udiv i32 %a, %b
-  ret i32 %div
-}
-
-; CHECK-LABEL: @Div
-; CHECK: icmp
-; CHECK: call void @__msan_warning
-; CHECK-NOT: icmp
-; CHECK: udiv
-; CHECK-NOT: icmp
-; CHECK: ret i32
-
-
-; Check that we propagate shadow for x<0, x>=0, etc (i.e. sign bit tests)
-
-define zeroext i1 @ICmpSLTZero(i32 %x) nounwind uwtable readnone sanitize_memory {
-  %1 = icmp slt i32 %x, 0
-  ret i1 %1
-}
-
-; CHECK-LABEL: @ICmpSLTZero
-; CHECK: icmp slt
-; CHECK-NOT: call void @__msan_warning
-; CHECK: icmp slt
-; CHECK-NOT: call void @__msan_warning
-; CHECK: ret i1
-
-define zeroext i1 @ICmpSGEZero(i32 %x) nounwind uwtable readnone sanitize_memory {
-  %1 = icmp sge i32 %x, 0
-  ret i1 %1
-}
-
-; CHECK-LABEL: @ICmpSGEZero
-; CHECK: icmp slt
-; CHECK-NOT: call void @__msan_warning
-; CHECK: icmp sge
-; CHECK-NOT: call void @__msan_warning
-; CHECK: ret i1
-
-define zeroext i1 @ICmpSGTZero(i32 %x) nounwind uwtable readnone sanitize_memory {
-  %1 = icmp sgt i32 0, %x
-  ret i1 %1
-}
-
-; CHECK-LABEL: @ICmpSGTZero
-; CHECK: icmp slt
-; CHECK-NOT: call void @__msan_warning
-; CHECK: icmp sgt
-; CHECK-NOT: call void @__msan_warning
-; CHECK: ret i1
-
-define zeroext i1 @ICmpSLEZero(i32 %x) nounwind uwtable readnone sanitize_memory {
-  %1 = icmp sle i32 0, %x
-  ret i1 %1
-}
-
-; CHECK-LABEL: @ICmpSLEZero
-; CHECK: icmp slt
-; CHECK-NOT: call void @__msan_warning
-; CHECK: icmp sle
-; CHECK-NOT: call void @__msan_warning
-; CHECK: ret i1
-
-
-; Check that we propagate shadow for x<=-1, x>-1, etc (i.e. sign bit tests)
-
-define zeroext i1 @ICmpSLTAllOnes(i32 %x) nounwind uwtable readnone sanitize_memory {
-  %1 = icmp slt i32 -1, %x
-  ret i1 %1
-}
-
-; CHECK-LABEL: @ICmpSLTAllOnes
-; CHECK: icmp slt
-; CHECK-NOT: call void @__msan_warning
-; CHECK: icmp slt
-; CHECK-NOT: call void @__msan_warning
-; CHECK: ret i1
-
-define zeroext i1 @ICmpSGEAllOnes(i32 %x) nounwind uwtable readnone sanitize_memory {
-  %1 = icmp sge i32 -1, %x
-  ret i1 %1
-}
-
-; CHECK-LABEL: @ICmpSGEAllOnes
-; CHECK: icmp slt
-; CHECK-NOT: call void @__msan_warning
-; CHECK: icmp sge
-; CHECK-NOT: call void @__msan_warning
-; CHECK: ret i1
-
-define zeroext i1 @ICmpSGTAllOnes(i32 %x) nounwind uwtable readnone sanitize_memory {
-  %1 = icmp sgt i32 %x, -1
-  ret i1 %1
-}
-
-; CHECK-LABEL: @ICmpSGTAllOnes
-; CHECK: icmp slt
-; CHECK-NOT: call void @__msan_warning
-; CHECK: icmp sgt
-; CHECK-NOT: call void @__msan_warning
-; CHECK: ret i1
-
-define zeroext i1 @ICmpSLEAllOnes(i32 %x) nounwind uwtable readnone sanitize_memory {
-  %1 = icmp sle i32 %x, -1
-  ret i1 %1
-}
-
-; CHECK-LABEL: @ICmpSLEAllOnes
-; CHECK: icmp slt
-; CHECK-NOT: call void @__msan_warning
-; CHECK: icmp sle
-; CHECK-NOT: call void @__msan_warning
-; CHECK: ret i1
-
-
-; Check that we propagate shadow for x<0, x>=0, etc (i.e. sign bit tests)
-; of the vector arguments.
-
-define <2 x i1> @ICmpSLT_vector_Zero(<2 x i32*> %x) nounwind uwtable readnone sanitize_memory {
-  %1 = icmp slt <2 x i32*> %x, zeroinitializer
-  ret <2 x i1> %1
-}
-
-; CHECK-LABEL: @ICmpSLT_vector_Zero
-; CHECK: icmp slt <2 x i64>
-; CHECK-NOT: call void @__msan_warning
-; CHECK: icmp slt <2 x i32*>
-; CHECK-NOT: call void @__msan_warning
-; CHECK: ret <2 x i1>
-
-; Check that we propagate shadow for x<=-1, x>0, etc (i.e. sign bit tests)
-; of the vector arguments.
-
-define <2 x i1> @ICmpSLT_vector_AllOnes(<2 x i32> %x) nounwind uwtable readnone sanitize_memory {
-  %1 = icmp slt <2 x i32> <i32 -1, i32 -1>, %x
-  ret <2 x i1> %1
-}
-
-; CHECK-LABEL: @ICmpSLT_vector_AllOnes
-; CHECK: icmp slt <2 x i32>
-; CHECK-NOT: call void @__msan_warning
-; CHECK: icmp slt <2 x i32>
-; CHECK-NOT: call void @__msan_warning
-; CHECK: ret <2 x i1>
-
-
-; Check that we propagate shadow for unsigned relational comparisons with
-; constants
-
-define zeroext i1 @ICmpUGTConst(i32 %x) nounwind uwtable readnone sanitize_memory {
-entry:
-  %cmp = icmp ugt i32 %x, 7
-  ret i1 %cmp
-}
-
-; CHECK-LABEL: @ICmpUGTConst
-; CHECK: icmp ugt i32
-; CHECK-NOT: call void @__msan_warning
-; CHECK: icmp ugt i32
-; CHECK-NOT: call void @__msan_warning
-; CHECK: icmp ugt i32
-; CHECK-NOT: call void @__msan_warning
-; CHECK: ret i1
-
-
-; Check that loads of shadow have the same alignment as the original loads.
-; Check that loads of origin have the alignment of max(4, original alignment).
-
-define i32 @ShadowLoadAlignmentLarge() nounwind uwtable sanitize_memory {
-  %y = alloca i32, align 64
-  %1 = load volatile i32, i32* %y, align 64
-  ret i32 %1
-}
-
-; CHECK-LABEL: @ShadowLoadAlignmentLarge
-; CHECK: load volatile i32, i32* {{.*}} align 64
-; CHECK: load i32, i32* {{.*}} align 64
-; CHECK: ret i32
-
-define i32 @ShadowLoadAlignmentSmall() nounwind uwtable sanitize_memory {
-  %y = alloca i32, align 2
-  %1 = load volatile i32, i32* %y, align 2
-  ret i32 %1
-}
-
-; CHECK-LABEL: @ShadowLoadAlignmentSmall
-; CHECK: load volatile i32, i32* {{.*}} align 2
-; CHECK: load i32, i32* {{.*}} align 2
-; CHECK-ORIGINS: load i32, i32* {{.*}} align 4
-; CHECK: ret i32
-
-
-; Test vector manipulation instructions.
-; Check that the same bit manipulation is applied to the shadow values.
-; Check that there is a zero test of the shadow of %idx argument, where present.
-
-define i32 @ExtractElement(<4 x i32> %vec, i32 %idx) sanitize_memory {
-  %x = extractelement <4 x i32> %vec, i32 %idx
-  ret i32 %x
-}
-
-; CHECK-LABEL: @ExtractElement
-; CHECK: extractelement
-; CHECK: call void @__msan_warning
-; CHECK: extractelement
-; CHECK: ret i32
-
-define <4 x i32> @InsertElement(<4 x i32> %vec, i32 %idx, i32 %x) sanitize_memory {
-  %vec1 = insertelement <4 x i32> %vec, i32 %x, i32 %idx
-  ret <4 x i32> %vec1
-}
-
-; CHECK-LABEL: @InsertElement
-; CHECK: insertelement
-; CHECK: call void @__msan_warning
-; CHECK: insertelement
-; CHECK: ret <4 x i32>
-
-define <4 x i32> @ShuffleVector(<4 x i32> %vec, <4 x i32> %vec1) sanitize_memory {
-  %vec2 = shufflevector <4 x i32> %vec, <4 x i32> %vec1,
-                        <4 x i32> <i32 0, i32 4, i32 1, i32 5>
-  ret <4 x i32> %vec2
-}
-
-; CHECK-LABEL: @ShuffleVector
-; CHECK: shufflevector
-; CHECK-NOT: call void @__msan_warning
-; CHECK: shufflevector
-; CHECK: ret <4 x i32>
-
-
-; Test bswap intrinsic instrumentation
-define i32 @BSwap(i32 %x) nounwind uwtable readnone sanitize_memory {
-  %y = tail call i32 @llvm.bswap.i32(i32 %x)
-  ret i32 %y
-}
-
-declare i32 @llvm.bswap.i32(i32) nounwind readnone
-
-; CHECK-LABEL: @BSwap
-; CHECK-NOT: call void @__msan_warning
-; CHECK: @llvm.bswap.i32
-; CHECK-NOT: call void @__msan_warning
-; CHECK: @llvm.bswap.i32
-; CHECK-NOT: call void @__msan_warning
-; CHECK: ret i32
-
-; Test handling of vectors of pointers.
-; Check that shadow of such vector is a vector of integers.
-
-define <8 x i8*> @VectorOfPointers(<8 x i8*>* %p) nounwind uwtable sanitize_memory {
-  %x = load <8 x i8*>, <8 x i8*>* %p
-  ret <8 x i8*> %x
-}
-
-; CHECK-LABEL: @VectorOfPointers
-; CHECK: load <8 x i8*>, <8 x i8*>*
-; CHECK: load <8 x i64>, <8 x i64>*
-; CHECK: store <8 x i64> {{.*}} @__msan_retval_tls
-; CHECK: ret <8 x i8*>
-
-; Test handling of va_copy.
-
-declare void @llvm.va_copy(i8*, i8*) nounwind
-
-define void @VACopy(i8* %p1, i8* %p2) nounwind uwtable sanitize_memory {
-  call void @llvm.va_copy(i8* %p1, i8* %p2) nounwind
-  ret void
-}
-
-; CHECK-LABEL: @VACopy
-; CHECK: call void @llvm.memset.p0i8.i64({{.*}}, i8 0, i64 24, i32 8, i1 false)
-; CHECK: ret void
-
-
-; Test that va_start instrumentation does not use va_arg_tls*.
-; It should work with a local stack copy instead.
-
-%struct.__va_list_tag = type { i32, i32, i8*, i8* }
-declare void @llvm.va_start(i8*) nounwind
-
-; Function Attrs: nounwind uwtable
-define void @VAStart(i32 %x, ...) sanitize_memory {
-entry:
-  %x.addr = alloca i32, align 4
-  %va = alloca [1 x %struct.__va_list_tag], align 16
-  store i32 %x, i32* %x.addr, align 4
-  %arraydecay = getelementptr inbounds [1 x %struct.__va_list_tag], [1 x %struct.__va_list_tag]* %va, i32 0, i32 0
-  %arraydecay1 = bitcast %struct.__va_list_tag* %arraydecay to i8*
-  call void @llvm.va_start(i8* %arraydecay1)
-  ret void
-}
-
-; CHECK-LABEL: @VAStart
-; CHECK: call void @llvm.va_start
-; CHECK-NOT: @__msan_va_arg_tls
-; CHECK-NOT: @__msan_va_arg_overflow_size_tls
-; CHECK: ret void
-
-
-; Test handling of volatile stores.
-; Check that MemorySanitizer does not add a check of the value being stored.
-
-define void @VolatileStore(i32* nocapture %p, i32 %x) nounwind uwtable sanitize_memory {
-entry:
-  store volatile i32 %x, i32* %p, align 4
-  ret void
-}
-
-; CHECK-LABEL: @VolatileStore
-; CHECK-NOT: @__msan_warning
-; CHECK: ret void
-
-
-; Test that checks are omitted and returned value is always initialized if
-; sanitize_memory attribute is missing.
-
-define i32 @NoSanitizeMemory(i32 %x) uwtable {
-entry:
-  %tobool = icmp eq i32 %x, 0
-  br i1 %tobool, label %if.end, label %if.then
-
-if.then:                                          ; preds = %entry
-  tail call void @bar()
-  br label %if.end
-
-if.end:                                           ; preds = %entry, %if.then
-  ret i32 %x
-}
-
-declare void @bar()
-
-; CHECK-LABEL: @NoSanitizeMemory
-; CHECK-NOT: @__msan_warning
-; CHECK: store i32 0, {{.*}} @__msan_retval_tls
-; CHECK-NOT: @__msan_warning
-; CHECK: ret i32
-
-
-; Test that stack allocations are unpoisoned in functions missing
-; sanitize_memory attribute
-
-define i32 @NoSanitizeMemoryAlloca() {
-entry:
-  %p = alloca i32, align 4
-  %x = call i32 @NoSanitizeMemoryAllocaHelper(i32* %p)
-  ret i32 %x
-}
-
-declare i32 @NoSanitizeMemoryAllocaHelper(i32* %p)
-
-; CHECK-LABEL: @NoSanitizeMemoryAlloca
-; CHECK: call void @llvm.memset.p0i8.i64(i8* {{.*}}, i8 0, i64 4, i32 4, i1 false)
-; CHECK: call i32 @NoSanitizeMemoryAllocaHelper(i32*
-; CHECK: ret i32
-
-
-; Test that undef is unpoisoned in functions missing
-; sanitize_memory attribute
-
-define i32 @NoSanitizeMemoryUndef() {
-entry:
-  %x = call i32 @NoSanitizeMemoryUndefHelper(i32 undef)
-  ret i32 %x
-}
-
-declare i32 @NoSanitizeMemoryUndefHelper(i32 %x)
-
-; CHECK-LABEL: @NoSanitizeMemoryUndef
-; CHECK: store i32 0, i32* {{.*}} @__msan_param_tls
-; CHECK: call i32 @NoSanitizeMemoryUndefHelper(i32 undef)
-; CHECK: ret i32
-
-
-; Test PHINode instrumentation in blacklisted functions
-
-define i32 @NoSanitizeMemoryPHI(i32 %x) {
-entry:
-  %tobool = icmp ne i32 %x, 0
-  br i1 %tobool, label %cond.true, label %cond.false
-
-cond.true:                                        ; preds = %entry
-  br label %cond.end
-
-cond.false:                                       ; preds = %entry
-  br label %cond.end
-
-cond.end:                                         ; preds = %cond.false, %cond.true
-  %cond = phi i32 [ undef, %cond.true ], [ undef, %cond.false ]
-  ret i32 %cond
-}
-
-; CHECK: [[A:%.*]] = phi i32 [ undef, %cond.true ], [ undef, %cond.false ]
-; CHECK: store i32 0, i32* bitcast {{.*}} @__msan_retval_tls
-; CHECK: ret i32 [[A]]
-
-
-; Test that there are no __msan_param_origin_tls stores when
-; argument shadow is a compile-time zero constant (which is always the case
-; in functions missing sanitize_memory attribute).
-
-define i32 @NoSanitizeMemoryParamTLS(i32* nocapture readonly %x) {
-entry:
-  %0 = load i32, i32* %x, align 4
-  %call = tail call i32 @NoSanitizeMemoryParamTLSHelper(i32 %0)
-  ret i32 %call
-}
-
-declare i32 @NoSanitizeMemoryParamTLSHelper(i32 %x)
-
-; CHECK-LABEL: define i32 @NoSanitizeMemoryParamTLS(
-; CHECK-NOT: __msan_param_origin_tls
-; CHECK: ret i32
-
-
-; Test argument shadow alignment
-
-define <2 x i64> @ArgumentShadowAlignment(i64 %a, <2 x i64> %b) sanitize_memory {
-entry:
-  ret <2 x i64> %b
-}
-
-; CHECK-LABEL: @ArgumentShadowAlignment
-; CHECK: load <2 x i64>, <2 x i64>* {{.*}} @__msan_param_tls {{.*}}, align 8
-; CHECK: store <2 x i64> {{.*}} @__msan_retval_tls {{.*}}, align 8
-; CHECK: ret <2 x i64>
-
-
-; Test origin propagation for insertvalue
-
-define { i64, i32 } @make_pair_64_32(i64 %x, i32 %y) sanitize_memory {
-entry:
-  %a = insertvalue { i64, i32 } undef, i64 %x, 0
-  %b = insertvalue { i64, i32 } %a, i32 %y, 1
-  ret { i64, i32 } %b
-}
-
-; CHECK-ORIGINS: @make_pair_64_32
-; First element shadow
-; CHECK-ORIGINS: insertvalue { i64, i32 } { i64 -1, i32 -1 }, i64 {{.*}}, 0
-; First element origin
-; CHECK-ORIGINS: icmp ne i64
-; CHECK-ORIGINS: select i1
-; First element app value
-; CHECK-ORIGINS: insertvalue { i64, i32 } undef, i64 {{.*}}, 0
-; Second element shadow
-; CHECK-ORIGINS: insertvalue { i64, i32 } {{.*}}, i32 {{.*}}, 1
-; Second element origin
-; CHECK-ORIGINS: icmp ne i32
-; CHECK-ORIGINS: select i1
-; Second element app value
-; CHECK-ORIGINS: insertvalue { i64, i32 } {{.*}}, i32 {{.*}}, 1
-; CHECK-ORIGINS: ret { i64, i32 }
-
-
-; Test shadow propagation for aggregates passed through ellipsis.
-
-%struct.StructByVal = type { i32, i32, i32, i32 }
-
-declare void @VAArgStructFn(i32 %guard, ...)
-
-define void @VAArgStruct(%struct.StructByVal* nocapture %s) sanitize_memory {
-entry:
-  %agg.tmp2 = alloca %struct.StructByVal, align 8
-  %0 = bitcast %struct.StructByVal* %s to i8*
-  %agg.tmp.sroa.0.0..sroa_cast = bitcast %struct.StructByVal* %s to i64*
-  %agg.tmp.sroa.0.0.copyload = load i64, i64* %agg.tmp.sroa.0.0..sroa_cast, align 4
-  %agg.tmp.sroa.2.0..sroa_idx = getelementptr inbounds %struct.StructByVal, %struct.StructByVal* %s, i64 0, i32 2
-  %agg.tmp.sroa.2.0..sroa_cast = bitcast i32* %agg.tmp.sroa.2.0..sroa_idx to i64*
-  %agg.tmp.sroa.2.0.copyload = load i64, i64* %agg.tmp.sroa.2.0..sroa_cast, align 4
-  %1 = bitcast %struct.StructByVal* %agg.tmp2 to i8*
-  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %1, i8* %0, i64 16, i32 4, i1 false)
-  call void (i32, ...) @VAArgStructFn(i32 undef, i64 %agg.tmp.sroa.0.0.copyload, i64 %agg.tmp.sroa.2.0.copyload, i64 %agg.tmp.sroa.0.0.copyload, i64 %agg.tmp.sroa.2.0.copyload, %struct.StructByVal* byval align 8 %agg.tmp2)
-  ret void
-}
-
-; "undef" and the first 2 structs go to general purpose registers;
-; the third struct goes to the overflow area byval
-
-; CHECK-LABEL: @VAArgStruct
-; undef not stored to __msan_va_arg_tls - it's a fixed argument
-; first struct through general purpose registers
-; CHECK: store i64 {{.*}}, i64* {{.*}}@__msan_va_arg_tls{{.*}}, i64 8){{.*}}, align 8
-; CHECK: store i64 {{.*}}, i64* {{.*}}@__msan_va_arg_tls{{.*}}, i64 16){{.*}}, align 8
-; second struct through general purpose registers
-; CHECK: store i64 {{.*}}, i64* {{.*}}@__msan_va_arg_tls{{.*}}, i64 24){{.*}}, align 8
-; CHECK: store i64 {{.*}}, i64* {{.*}}@__msan_va_arg_tls{{.*}}, i64 32){{.*}}, align 8
-; third struct through the overflow area byval
-; CHECK: ptrtoint %struct.StructByVal* {{.*}} to i64
-; CHECK: bitcast { i32, i32, i32, i32 }* {{.*}}@__msan_va_arg_tls {{.*}}, i64 176
-; CHECK: call void @llvm.memcpy.p0i8.p0i8.i64
-; CHECK: store i64 16, i64* @__msan_va_arg_overflow_size_tls
-; CHECK: call void (i32, ...) @VAArgStructFn
-; CHECK: ret void
-
-
-declare i32 @InnerTailCall(i32 %a)
-
-define void @MismatchedReturnTypeTailCall(i32 %a) sanitize_memory {
-  %b = tail call i32 @InnerTailCall(i32 %a)
-  ret void
-}
-
-; We used to strip off the 'tail' modifier, but now that we unpoison return slot
-; shadow before the call, we don't need to anymore.
-
-; CHECK-LABEL: define void @MismatchedReturnTypeTailCall
-; CHECK: tail call i32 @InnerTailCall
-; CHECK: ret void
-
-
-declare i32 @MustTailCall(i32 %a)
-
-define i32 @CallMustTailCall(i32 %a) sanitize_memory {
-  %b = musttail call i32 @MustTailCall(i32 %a)
-  ret i32 %b
-}
-
-; For "musttail" calls we can not insert any shadow manipulating code between
-; call and the return instruction. And we don't need to, because everything is
-; taken care of in the callee.
-
-; CHECK-LABEL: define i32 @CallMustTailCall
-; CHECK: musttail call i32 @MustTailCall
-; No instrumentation between call and ret.
-; CHECK-NEXT: ret i32
-
-declare i32* @MismatchingMustTailCall(i32 %a)
-
-define i8* @MismatchingCallMustTailCall(i32 %a) sanitize_memory {
-  %b = musttail call i32* @MismatchingMustTailCall(i32 %a)
-  %c = bitcast i32* %b to i8*
-  ret i8* %c
-}
-
-; For "musttail" calls we can not insert any shadow manipulating code between
-; call and the return instruction. And we don't need to, because everything is
-; taken care of in the callee.
-
-; CHECK-LABEL: define i8* @MismatchingCallMustTailCall
-; CHECK: musttail call i32* @MismatchingMustTailCall
-; No instrumentation between call and ret.
-; CHECK-NEXT: bitcast i32* {{.*}} to i8*
-; CHECK-NEXT: ret i8*
-
-
-; CHECK-LABEL: define internal void @msan.module_ctor() {
-; CHECK: call void @__msan_init()
Index: test/Instrumentation/MemorySanitizer/msan_common_basic.ll
===================================================================
--- test/Instrumentation/MemorySanitizer/msan_common_basic.ll	(revision 0)
+++ test/Instrumentation/MemorySanitizer/msan_common_basic.ll	(working copy)
@@ -0,0 +1,531 @@
+; RUN: opt < %s -msan -msan-check-access-address=0 -S | FileCheck -check-prefix=CHECK -check-prefix=CHECK-MSAN %s
+; RUN: opt < %s -msan -msan-check-access-address=0 -msan-track-origins=1 -S | FileCheck -check-prefix=CHECK -check-prefix=CHECK-ORIGINS -check-prefix=CHECK-MSAN %s
+; RUN: opt < %s -msan -msan-kernel=1 -S | FileCheck %s -check-prefix=CHECK -check-prefix=CHECK-KMSAN -check-prefix=CHECK-ORIGINS
+
+target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
+target triple = "x86_64-unknown-linux-gnu"
+
+; Check that we generate PHIs for shadow.
+define void @FuncWithPhi(i32* nocapture %a, i32* %b, i32* nocapture %c) nounwind uwtable sanitize_memory {
+entry:
+  %tobool = icmp eq i32* %b, null
+  br i1 %tobool, label %if.else, label %if.then
+
+  if.then:                                          ; preds = %entry
+  %0 = load i32, i32* %b, align 4
+  br label %if.end
+
+  if.else:                                          ; preds = %entry
+  %1 = load i32, i32* %c, align 4
+  br label %if.end
+
+  if.end:                                           ; preds = %if.else, %if.then
+  %t.0 = phi i32 [ %0, %if.then ], [ %1, %if.else ]
+  store i32 %t.0, i32* %a, align 4
+  ret void
+}
+
+; CHECK-LABEL: @FuncWithPhi
+; CHECK: = phi
+; CHECK-NEXT: = phi
+; CHECK: store
+; CHECK: store
+; CHECK: ret void
+
+
+; Compute shadow for "x << 10"
+define void @ShlConst(i32* nocapture %x) nounwind uwtable sanitize_memory {
+entry:
+  %0 = load i32, i32* %x, align 4
+  %1 = shl i32 %0, 10
+  store i32 %1, i32* %x, align 4
+  ret void
+}
+
+; CHECK-LABEL: @ShlConst
+; CHECK: = load
+; CHECK: = load
+; CHECK: shl
+; CHECK: shl
+; CHECK: store
+; CHECK: store
+; CHECK: ret void
+
+; Compute shadow for "10 << x": it should have 'sext i1'.
+define void @ShlNonConst(i32* nocapture %x) nounwind uwtable sanitize_memory {
+entry:
+  %0 = load i32, i32* %x, align 4
+  %1 = shl i32 10, %0
+  store i32 %1, i32* %x, align 4
+  ret void
+}
+
+; CHECK-LABEL: @ShlNonConst
+; CHECK: = load
+; CHECK: = load
+; CHECK: = sext i1
+; CHECK: store
+; CHECK: store
+; CHECK: ret void
+
+; SExt
+define void @SExt(i32* nocapture %a, i16* nocapture %b) nounwind uwtable sanitize_memory {
+entry:
+  %0 = load i16, i16* %b, align 2
+  %1 = sext i16 %0 to i32
+  store i32 %1, i32* %a, align 4
+  ret void
+}
+
+; CHECK-LABEL: @SExt
+; CHECK: = load
+; CHECK: = load
+; CHECK: = sext
+; CHECK: = sext
+; CHECK: store
+; CHECK: store
+; CHECK: ret void
+
+
+; memset
+define void @MemSet(i8* nocapture %x) nounwind uwtable sanitize_memory {
+entry:
+  call void @llvm.memset.p0i8.i64(i8* %x, i8 42, i64 10, i32 1, i1 false)
+  ret void
+}
+
+declare void @llvm.memset.p0i8.i64(i8* nocapture, i8, i64, i32, i1) nounwind
+
+; CHECK-LABEL: @MemSet
+; CHECK: call i8* @__msan_memset
+; CHECK: ret void
+
+
+; memcpy
+define void @MemCpy(i8* nocapture %x, i8* nocapture %y) nounwind uwtable sanitize_memory {
+entry:
+  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %x, i8* %y, i64 10, i32 1, i1 false)
+  ret void
+}
+
+declare void @llvm.memcpy.p0i8.p0i8.i64(i8* nocapture, i8* nocapture, i64, i32, i1) nounwind
+
+; CHECK-LABEL: @MemCpy
+; CHECK: call i8* @__msan_memcpy
+; CHECK: ret void
+
+
+; memmove is lowered to a call
+define void @MemMove(i8* nocapture %x, i8* nocapture %y) nounwind uwtable sanitize_memory {
+entry:
+  call void @llvm.memmove.p0i8.p0i8.i64(i8* %x, i8* %y, i64 10, i32 1, i1 false)
+  ret void
+}
+
+declare void @llvm.memmove.p0i8.p0i8.i64(i8* nocapture, i8* nocapture, i64, i32, i1) nounwind
+
+; CHECK-LABEL: @MemMove
+; CHECK: call i8* @__msan_memmove
+; CHECK: ret void
+
+
+;; ------------
+;; Placeholder tests that will fail once element atomic @llvm.mem[cpy|move|set] instrinsics have
+;; been added to the MemIntrinsic class hierarchy. These will act as a reminder to
+;; verify that MSAN handles these intrinsics properly once they have been
+;; added to that class hierarchy.
+declare void @llvm.memset.element.unordered.atomic.p0i8.i64(i8* nocapture writeonly, i8, i64, i32) nounwind
+declare void @llvm.memmove.element.unordered.atomic.p0i8.p0i8.i64(i8* nocapture writeonly, i8* nocapture readonly, i64, i32) nounwind
+declare void @llvm.memcpy.element.unordered.atomic.p0i8.p0i8.i64(i8* nocapture writeonly, i8* nocapture readonly, i64, i32) nounwind
+
+define void @atomic_memcpy(i8* nocapture %x, i8* nocapture %y) nounwind {
+  ; CHECK-LABEL: atomic_memcpy
+  ; CHECK-KMSAN: .split:
+  ; CHECK-NEXT: call void @llvm.memcpy.element.unordered.atomic.p0i8.p0i8.i64(i8* align 1 %x, i8* align 2 %y, i64 16, i32 1)
+  ; CHECK-NEXT: ret void
+  call void @llvm.memcpy.element.unordered.atomic.p0i8.p0i8.i64(i8* align 1 %x, i8* align 2 %y, i64 16, i32 1)
+  ret void
+}
+
+define void @atomic_memmove(i8* nocapture %x, i8* nocapture %y) nounwind {
+  ; CHECK-LABEL: atomic_memmove
+  ; CHECK-KMSAN: .split:
+  ; CHECK-NEXT: call void @llvm.memmove.element.unordered.atomic.p0i8.p0i8.i64(i8* align 1 %x, i8* align 2 %y, i64 16, i32 1)
+  ; CHECK-NEXT: ret void
+  call void @llvm.memmove.element.unordered.atomic.p0i8.p0i8.i64(i8* align 1 %x, i8* align 2 %y, i64 16, i32 1)
+  ret void
+}
+
+define void @atomic_memset(i8* nocapture %x) nounwind {
+  ; CHECK-LABEL: atomic_memset
+  ; CHECK-KMSAN: .split:
+  ; CHECK-NEXT: call void @llvm.memset.element.unordered.atomic.p0i8.i64(i8* align 1 %x, i8 88, i64 16, i32 1)
+  ; CHECK-NEXT: ret void
+  call void @llvm.memset.element.unordered.atomic.p0i8.i64(i8* align 1 %x, i8 88, i64 16, i32 1)
+  ret void
+}
+
+;; ------------
+
+; Check that we insert exactly one check on udiv
+; (2nd arg shadow is checked, 1st arg shadow is propagated)
+
+define i32 @Div(i32 %a, i32 %b) nounwind uwtable readnone sanitize_memory {
+entry:
+  %div = udiv i32 %a, %b
+  ret i32 %div
+}
+
+; CHECK-LABEL: @Div
+; CHECK: icmp
+; CHECK: call void @__msan_warning{{.*}}
+; CHECK-NOT: icmp
+; CHECK: udiv
+; CHECK-NOT: icmp
+; CHECK: ret i32
+
+
+; Check that we propagate shadow for x<0, x>=0, etc (i.e. sign bit tests)
+
+define zeroext i1 @ICmpSLTZero(i32 %x) nounwind uwtable readnone sanitize_memory {
+  %1 = icmp slt i32 %x, 0
+  ret i1 %1
+}
+
+; CHECK-LABEL: @ICmpSLTZero
+; CHECK: icmp slt
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: icmp slt
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: ret i1
+
+define zeroext i1 @ICmpSGEZero(i32 %x) nounwind uwtable readnone sanitize_memory {
+  %1 = icmp sge i32 %x, 0
+  ret i1 %1
+}
+
+; CHECK-LABEL: @ICmpSGEZero
+; CHECK: icmp slt
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: icmp sge
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: ret i1
+
+define zeroext i1 @ICmpSGTZero(i32 %x) nounwind uwtable readnone sanitize_memory {
+  %1 = icmp sgt i32 0, %x
+  ret i1 %1
+}
+
+; CHECK-LABEL: @ICmpSGTZero
+; CHECK: icmp slt
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: icmp sgt
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: ret i1
+
+define zeroext i1 @ICmpSLEZero(i32 %x) nounwind uwtable readnone sanitize_memory {
+  %1 = icmp sle i32 0, %x
+  ret i1 %1
+}
+
+; CHECK-LABEL: @ICmpSLEZero
+; CHECK: icmp slt
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: icmp sle
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: ret i1
+
+
+; Check that we propagate shadow for x<=-1, x>-1, etc (i.e. sign bit tests)
+
+define zeroext i1 @ICmpSLTAllOnes(i32 %x) nounwind uwtable readnone sanitize_memory {
+  %1 = icmp slt i32 -1, %x
+  ret i1 %1
+}
+
+; CHECK-LABEL: @ICmpSLTAllOnes
+; CHECK: icmp slt
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: icmp slt
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: ret i1
+
+define zeroext i1 @ICmpSGEAllOnes(i32 %x) nounwind uwtable readnone sanitize_memory {
+  %1 = icmp sge i32 -1, %x
+  ret i1 %1
+}
+
+; CHECK-LABEL: @ICmpSGEAllOnes
+; CHECK: icmp slt
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: icmp sge
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: ret i1
+
+define zeroext i1 @ICmpSGTAllOnes(i32 %x) nounwind uwtable readnone sanitize_memory {
+  %1 = icmp sgt i32 %x, -1
+  ret i1 %1
+}
+
+; CHECK-LABEL: @ICmpSGTAllOnes
+; CHECK: icmp slt
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: icmp sgt
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: ret i1
+
+define zeroext i1 @ICmpSLEAllOnes(i32 %x) nounwind uwtable readnone sanitize_memory {
+  %1 = icmp sle i32 %x, -1
+  ret i1 %1
+}
+
+; CHECK-LABEL: @ICmpSLEAllOnes
+; CHECK: icmp slt
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: icmp sle
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: ret i1
+
+
+; Check that we propagate shadow for x<0, x>=0, etc (i.e. sign bit tests)
+; of the vector arguments.
+
+define <2 x i1> @ICmpSLT_vector_Zero(<2 x i32*> %x) nounwind uwtable readnone sanitize_memory {
+  %1 = icmp slt <2 x i32*> %x, zeroinitializer
+  ret <2 x i1> %1
+}
+
+; CHECK-LABEL: @ICmpSLT_vector_Zero
+; CHECK: icmp slt <2 x i64>
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: icmp slt <2 x i32*>
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: ret <2 x i1>
+
+; Check that we propagate shadow for x<=-1, x>0, etc (i.e. sign bit tests)
+; of the vector arguments.
+
+define <2 x i1> @ICmpSLT_vector_AllOnes(<2 x i32> %x) nounwind uwtable readnone sanitize_memory {
+  %1 = icmp slt <2 x i32> <i32 -1, i32 -1>, %x
+  ret <2 x i1> %1
+}
+
+; CHECK-LABEL: @ICmpSLT_vector_AllOnes
+; CHECK: icmp slt <2 x i32>
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: icmp slt <2 x i32>
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: ret <2 x i1>
+
+; Test handling of va_copy.
+
+declare void @llvm.va_copy(i8*, i8*) nounwind
+
+define void @VACopy(i8* %p1, i8* %p2) nounwind uwtable sanitize_memory {
+  call void @llvm.va_copy(i8* %p1, i8* %p2) nounwind
+  ret void
+}
+
+; CHECK-LABEL: @VACopy
+; CHECK: call void @llvm.memset.p0i8.i64({{.*}}, i8 0, i64 24, i32 8, i1 false)
+; CHECK: ret void
+
+
+; Check that we propagate shadow for unsigned relational comparisons with
+; constants
+
+define zeroext i1 @ICmpUGTConst(i32 %x) nounwind uwtable readnone sanitize_memory {
+entry:
+  %cmp = icmp ugt i32 %x, 7
+  ret i1 %cmp
+}
+
+; CHECK-LABEL: @ICmpUGTConst
+; CHECK: icmp ugt i32
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: icmp ugt i32
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: icmp ugt i32
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: ret i1
+
+
+; Check that loads of shadow have the same alignment as the original loads.
+; Check that loads of origin have the alignment of max(4, original alignment).
+
+define i32 @ShadowLoadAlignmentLarge() nounwind uwtable sanitize_memory {
+  %y = alloca i32, align 64
+  %1 = load volatile i32, i32* %y, align 64
+  ret i32 %1
+}
+
+; CHECK-LABEL: @ShadowLoadAlignmentLarge
+; CHECK: load volatile i32, i32* {{.*}} align 64
+; CHECK: load i32, i32* {{.*}} align 64
+; CHECK: ret i32
+
+define i32 @ShadowLoadAlignmentSmall() nounwind uwtable sanitize_memory {
+  %y = alloca i32, align 2
+  %1 = load volatile i32, i32* %y, align 2
+  ret i32 %1
+}
+
+; CHECK-LABEL: @ShadowLoadAlignmentSmall
+; CHECK: load volatile i32, i32* {{.*}} align 2
+; CHECK: load i32, i32* {{.*}} align 2
+; CHECK-ORIGINS: load i32, i32* {{.*}} align 4
+; CHECK: ret i32
+
+
+; Test vector manipulation instructions.
+; Check that the same bit manipulation is applied to the shadow values.
+; Check that there is a zero test of the shadow of %idx argument, where present.
+
+define i32 @ExtractElement(<4 x i32> %vec, i32 %idx) sanitize_memory {
+  %x = extractelement <4 x i32> %vec, i32 %idx
+  ret i32 %x
+}
+
+; CHECK-LABEL: @ExtractElement
+; CHECK: extractelement
+; CHECK: call void @__msan_warning{{.*}}
+; CHECK: extractelement
+; CHECK: ret i32
+
+define <4 x i32> @InsertElement(<4 x i32> %vec, i32 %idx, i32 %x) sanitize_memory {
+  %vec1 = insertelement <4 x i32> %vec, i32 %x, i32 %idx
+  ret <4 x i32> %vec1
+}
+
+; CHECK-LABEL: @InsertElement
+; CHECK: insertelement
+; CHECK: call void @__msan_warning{{.*}}
+; CHECK: insertelement
+; CHECK: ret <4 x i32>
+
+define <4 x i32> @ShuffleVector(<4 x i32> %vec, <4 x i32> %vec1) sanitize_memory {
+  %vec2 = shufflevector <4 x i32> %vec, <4 x i32> %vec1,
+                        <4 x i32> <i32 0, i32 4, i32 1, i32 5>
+  ret <4 x i32> %vec2
+}
+
+; CHECK-LABEL: @ShuffleVector
+; CHECK: shufflevector
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: shufflevector
+; CHECK: ret <4 x i32>
+
+
+; Test bswap intrinsic instrumentation
+define i32 @BSwap(i32 %x) nounwind uwtable readnone sanitize_memory {
+  %y = tail call i32 @llvm.bswap.i32(i32 %x)
+  ret i32 %y
+}
+
+declare i32 @llvm.bswap.i32(i32) nounwind readnone
+
+; CHECK-LABEL: @BSwap
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: @llvm.bswap.i32
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: @llvm.bswap.i32
+; CHECK-NOT: call void @__msan_warning{{.*}}
+; CHECK: ret i32
+
+; Test handling of vectors of pointers.
+; Check that shadow of such vector is a vector of integers.
+
+define <8 x i8*> @VectorOfPointers(<8 x i8*>* %p) nounwind uwtable sanitize_memory {
+  %x = load <8 x i8*>, <8 x i8*>* %p
+  ret <8 x i8*> %x
+}
+
+; CHECK-LABEL: @VectorOfPointers
+; CHECK: load <8 x i8*>, <8 x i8*>*
+; CHECK: load <8 x i64>, <8 x i64>*
+; CHECK-MSAN: store <8 x i64> {{.*}} @__msan_retval_tls
+; CHECK-KMSAN: %retval_shadow
+; CHECK-KMSAN: store <8 x i64>
+; CHECK-KMSAN: store {{.*}} %retval_origin
+; CHECK: ret <8 x i8*>
+
+
+; Test origin propagation for insertvalue
+
+define { i64, i32 } @make_pair_64_32(i64 %x, i32 %y) sanitize_memory {
+entry:
+  %a = insertvalue { i64, i32 } undef, i64 %x, 0
+  %b = insertvalue { i64, i32 } %a, i32 %y, 1
+  ret { i64, i32 } %b
+}
+
+; CHECK-ORIGINS: @make_pair_64_32
+; First element shadow
+; CHECK-ORIGINS: insertvalue { i64, i32 } { i64 -1, i32 -1 }, i64 {{.*}}, 0
+; First element origin
+; CHECK-ORIGINS: icmp ne i64
+; CHECK-ORIGINS: select i1
+; First element app value
+; CHECK-ORIGINS: insertvalue { i64, i32 } undef, i64 {{.*}}, 0
+; Second element shadow
+; CHECK-ORIGINS: insertvalue { i64, i32 } {{.*}}, i32 {{.*}}, 1
+; Second element origin
+; CHECK-ORIGINS: icmp ne i32
+; CHECK-ORIGINS: select i1
+; Second element app value
+; CHECK-ORIGINS: insertvalue { i64, i32 } {{.*}}, i32 {{.*}}, 1
+; CHECK-ORIGINS: ret { i64, i32 }
+
+
+declare i32 @InnerTailCall(i32 %a)
+
+define void @MismatchedReturnTypeTailCall(i32 %a) sanitize_memory {
+  %b = tail call i32 @InnerTailCall(i32 %a)
+  ret void
+}
+
+; We used to strip off the 'tail' modifier, but now that we unpoison return slot
+; shadow before the call, we don't need to anymore.
+
+; CHECK-LABEL: define void @MismatchedReturnTypeTailCall
+; CHECK: tail call i32 @InnerTailCall
+; CHECK: ret void
+
+
+declare i32 @MustTailCall(i32 %a)
+
+define i32 @CallMustTailCall(i32 %a) sanitize_memory {
+  %b = musttail call i32 @MustTailCall(i32 %a)
+  ret i32 %b
+}
+
+; For "musttail" calls we can not insert any shadow manipulating code between
+; call and the return instruction. And we don't need to, because everything is
+; taken care of in the callee.
+
+; CHECK-LABEL: define i32 @CallMustTailCall
+; CHECK: musttail call i32 @MustTailCall
+; No instrumentation between call and ret.
+; CHECK-NEXT: ret i32
+
+declare i32* @MismatchingMustTailCall(i32 %a)
+
+define i8* @MismatchingCallMustTailCall(i32 %a) sanitize_memory {
+  %b = musttail call i32* @MismatchingMustTailCall(i32 %a)
+  %c = bitcast i32* %b to i8*
+  ret i8* %c
+}
+
+; For "musttail" calls we can not insert any shadow manipulating code between
+; call and the return instruction. And we don't need to, because everything is
+; taken care of in the callee.
+
+; CHECK-LABEL: define i8* @MismatchingCallMustTailCall
+; CHECK: musttail call i32* @MismatchingMustTailCall
+; No instrumentation between call and ret.
+; CHECK-NEXT: bitcast i32* {{.*}} to i8*
+; CHECK-NEXT: ret i8*
+
+
+; CHECK-LABEL: define internal void @msan.module_ctor() {
+; CHECK: call void @__msan_init()
Index: test/Instrumentation/MemorySanitizer/msan_kernel_basic.ll
===================================================================
--- test/Instrumentation/MemorySanitizer/msan_kernel_basic.ll	(revision 0)
+++ test/Instrumentation/MemorySanitizer/msan_kernel_basic.ll	(working copy)
@@ -0,0 +1,493 @@
+; RUN: opt < %s -msan -msan-kernel=1 -S | FileCheck %s -check-prefix=CHECK -check-prefix=CHECK-KMSAN -check-prefix=CHECK-ORIGINS
+
+target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
+target triple = "x86_64-unknown-linux-gnu"
+
+; Check the instrumentation prologue.
+define void @Empty() nounwind uwtable sanitize_memory {
+entry:
+  ret void
+}
+
+; CHECK-LABEL: @Empty
+; CHECK: entry:
+; CHECK: @__msan_get_context_state()
+; CHECK: %param_shadow
+; CHECK: %retval_shadow
+; CHECK: %va_arg_shadow
+; CHECK: %va_arg_overflow_size
+; CHECK: %param_origin
+; CHECK: %retval_origin
+; CHECK: entry.split:
+
+; Check instrumentation of stores
+
+define void @Store(i32* nocapture %p, i32 %x) nounwind uwtable sanitize_memory {
+entry:
+  store i32 %x, i32* %p, align 4
+  ret void
+}
+
+; CHECK-LABEL: @Store
+; CHECK: entry:
+; CHECK: @__msan_get_context_state()
+; CHECK: entry.split:
+; CHECK: %param_origin
+; Load the shadow of %p
+; CHECK: load
+; CHECK: [[REG:%[0-9]+]] = bitcast i32* %p to i8*
+; CHECK: @__msan_metadata_ptr_for_store_4(i8* [[REG]])
+; TODO: THIS STORE SHOULD GO BEFORE STORING THE ORIGIN
+; CHECK: store
+; CHECK: icmp
+; CHECK: br i1
+; CHECK: <label>
+; CHECK: @__msan_chain_origin
+; Storing origin here:
+; CHECK: store
+; CHECK: br label
+; CHECK: <label>
+; CHECK: store
+; CHECK: ret void
+
+
+; Check instrumentation of aligned stores
+; Shadow store has the same alignment as the original store; origin store
+; does not specify explicit alignment.
+
+define void @AlignedStore(i32* nocapture %p, i32 %x) nounwind uwtable sanitize_memory {
+entry:
+  store i32 %x, i32* %p, align 32
+  ret void
+}
+
+; CHECK-LABEL: @AlignedStore
+; CHECK: entry:
+; CHECK: @__msan_get_context_state()
+; CHECK: entry.split:
+; CHECK: store {{.*}} align 32
+; CHECK-ORIGINS: icmp
+; CHECK-ORIGINS: br i1
+; CHECK-ORIGINS: <label>
+; CHECK-ORIGINS: store {{.*}} align 32
+; CHECK-ORIGINS: br label
+; CHECK-ORIGINS: <label>
+; CHECK: store {{.*}} align 32
+; CHECK: ret void
+
+
+; load followed by cmp: check that we load the shadow and call __msan_warning_32().
+define void @LoadAndCmp(i32* nocapture %a) nounwind uwtable sanitize_memory {
+entry:
+  %0 = load i32, i32* %a, align 4
+  %tobool = icmp eq i32 %0, 0
+  br i1 %tobool, label %if.end, label %if.then
+
+if.then:                                          ; preds = %entry
+  tail call void (...) @foo() nounwind
+  br label %if.end
+
+if.end:                                           ; preds = %entry, %if.then
+  ret void
+}
+
+declare void @foo(...)
+
+; CHECK-LABEL: @LoadAndCmp
+; CHECK: = load
+; CHECK: = load
+; CHECK: call void @__msan_warning{{.*}}
+; CHECK: ret void
+
+; Check that we store the shadow for the retval.
+define i32 @ReturnInt() nounwind uwtable readnone sanitize_memory {
+entry:
+  ret i32 123
+}
+
+; CHECK-LABEL: @ReturnInt
+; CHECK: %retval_shadow
+; CHECK: store i32 0, {{.*}} align 8
+; CHECK: store i32 0, {{.*}} %retval_origin
+; CHECK: ret i32
+
+; Check that we get the shadow for the retval.
+define void @CopyRetVal(i32* nocapture %a) nounwind uwtable sanitize_memory {
+entry:
+  %call = tail call i32 @ReturnInt() nounwind
+  store i32 %call, i32* %a, align 4
+  ret void
+}
+
+; CHECK-LABEL: @CopyRetVal
+; Zero out the return value shadow before calling ReturnInt().
+; CHECK: %retval_shadow
+; Call ReturnInt()
+; CHECK: tail call i32 @ReturnInt
+; Load the shadow and origin for the return value
+; CHECK: %retval_shadow
+; CHECK: load
+; CHECK: load {{.*}} %retval_origin
+; Check the address shadow for |a| before writing to it.
+; CHECK: bitcast {{.*}} %a
+; CHECK: icmp
+; CHECK: br
+; CHECK: <label>
+; CHECK: call void @__msan_warning{{.*}}
+; CHECK: <label>
+; CHECK: @__msan_metadata_ptr_for_store_4
+; CHECK: <label>
+; CHECK: call i32 @__msan_chain_origin
+; CHECK: store
+; CHECK: <label>
+; CHECK: store
+; CHECK: ret void
+
+
+
+; Check that we propagate shadow for "select"
+
+define i32 @Select(i32 %a, i32 %b, i1 %c) nounwind uwtable readnone sanitize_memory {
+entry:
+  %cond = select i1 %c, i32 %a, i32 %b
+  ret i32 %cond
+}
+
+; CHECK-LABEL: @Select
+; CHECK: select i1
+; CHECK-DAG: or i32
+; CHECK-DAG: xor i32
+; CHECK: or i32
+; CHECK-DAG: select i1
+; CHECK-ORIGINS-DAG: select
+; CHECK-ORIGINS-DAG: select
+; CHECK-DAG: select i1
+; CHECK: %retval_shadow
+; CHECK: store i32
+; CHECK: store i32 {{.*}} %retval_origin
+; CHECK: ret i32
+
+
+; Check that we propagate origin for "select" with vector condition.
+; Select condition is flattened to i1, which is then used to select one of the
+; argument origins.
+
+define <8 x i16> @SelectVector(<8 x i16> %a, <8 x i16> %b, <8 x i1> %c) nounwind uwtable readnone sanitize_memory {
+entry:
+  %cond = select <8 x i1> %c, <8 x i16> %a, <8 x i16> %b
+  ret <8 x i16> %cond
+}
+
+; CHECK-LABEL: @SelectVector
+; CHECK: select <8 x i1>
+; CHECK-DAG: or <8 x i16>
+; CHECK-DAG: xor <8 x i16>
+; CHECK: or <8 x i16>
+; CHECK-DAG: select <8 x i1>
+; CHECK-ORIGINS-DAG: select
+; CHECK-ORIGINS-DAG: select
+; CHECK-DAG: select <8 x i1>
+; CHECK: %retval_shadow
+; CHECK: store <8 x i16>{{.*}}
+; CHECK: store i32 {{.*}} %retval_origin
+; CHECK: ret <8 x i16>
+
+
+; Check that we propagate origin for "select" with scalar condition and vector
+; arguments. Select condition shadow is sign-extended to the vector type and
+; mixed into the result shadow.
+
+define <8 x i16> @SelectVector2(<8 x i16> %a, <8 x i16> %b, i1 %c) nounwind uwtable readnone sanitize_memory {
+entry:
+  %cond = select i1 %c, <8 x i16> %a, <8 x i16> %b
+  ret <8 x i16> %cond
+}
+
+; CHECK-LABEL: @SelectVector2
+; CHECK: select i1
+; CHECK-DAG: or <8 x i16>
+; CHECK-DAG: xor <8 x i16>
+; CHECK: or <8 x i16>
+; CHECK-DAG: select i1
+; CHECK-ORIGINS-DAG: select i1
+; CHECK-ORIGINS-DAG: select i1
+; CHECK-DAG: select i1
+; CHECK: ret <8 x i16>
+
+
+define { i64, i64 } @SelectStruct(i1 zeroext %x, { i64, i64 } %a, { i64, i64 } %b) readnone sanitize_memory {
+entry:
+  %c = select i1 %x, { i64, i64 } %a, { i64, i64 } %b
+  ret { i64, i64 } %c
+}
+
+; CHECK-LABEL: @SelectStruct
+; CHECK: select i1 {{.*}}, { i64, i64 }
+; CHECK-NEXT: select i1 {{.*}}, { i64, i64 } { i64 -1, i64 -1 }, { i64, i64 }
+; CHECK-ORIGINS: select i1
+; CHECK-ORIGINS: select i1
+; CHECK-NEXT: select i1 {{.*}}, { i64, i64 }
+; CHECK: ret { i64, i64 }
+
+
+define { i64*, double } @SelectStruct2(i1 zeroext %x, { i64*, double } %a, { i64*, double } %b) readnone sanitize_memory {
+entry:
+  %c = select i1 %x, { i64*, double } %a, { i64*, double } %b
+  ret { i64*, double } %c
+}
+
+; CHECK-LABEL: @SelectStruct2
+; CHECK: select i1 {{.*}}, { i64, i64 }
+; CHECK-NEXT: select i1 {{.*}}, { i64, i64 } { i64 -1, i64 -1 }, { i64, i64 }
+; CHECK-ORIGINS: select i1
+; CHECK-ORIGINS: select i1
+; CHECK-NEXT: select i1 {{.*}}, { i64*, double }
+; CHECK: ret { i64*, double }
+
+
+define i8* @IntToPtr(i64 %x) nounwind uwtable readnone sanitize_memory {
+entry:
+  %0 = inttoptr i64 %x to i8*
+  ret i8* %0
+}
+
+; CHECK-LABEL: @IntToPtr
+; CHECK: %param_shadow
+; CHECK: load i64
+; CHECK-NEXT: %param_origin
+; CHECK: load
+; CHECK-NEXT: inttoptr
+; CHECK-NEXT: %retval_shadow
+; CHECK: store {{.*}} %retval_origin
+; CHECK: ret i8*
+
+
+define i8* @IntToPtr_ZExt(i16 %x) nounwind uwtable readnone sanitize_memory {
+entry:
+  %0 = inttoptr i16 %x to i8*
+  ret i8* %0
+}
+
+; CHECK-LABEL: @IntToPtr_ZExt
+; CHECK: %param_shadow
+; CHECK: load i16
+; CHECK: zext
+; CHECK-NEXT: inttoptr
+; CHECK-NEXT: %retval_shadow
+; CHECK: store
+; CHECK: store {{.*}} %retval_origin
+; CHECK: ret i8*
+
+
+; Test that va_start instrumentation does not use va_arg_tls*.
+; It should work with a local stack copy instead.
+
+%struct.__va_list_tag = type { i32, i32, i8*, i8* }
+declare void @llvm.va_start(i8*) nounwind
+
+; Function Attrs: nounwind uwtable
+define void @VAStart(i32 %x, ...) sanitize_memory {
+entry:
+  %x.addr = alloca i32, align 4
+  %va = alloca [1 x %struct.__va_list_tag], align 16
+  store i32 %x, i32* %x.addr, align 4
+  %arraydecay = getelementptr inbounds [1 x %struct.__va_list_tag], [1 x %struct.__va_list_tag]* %va, i32 0, i32 0
+  %arraydecay1 = bitcast %struct.__va_list_tag* %arraydecay to i8*
+  call void @llvm.va_start(i8* %arraydecay1)
+  ret void
+}
+
+; CHECK-LABEL: @VAStart
+; CHECK: call void @llvm.va_start
+; CHECK-NOT: @__msan_va_arg_tls
+; CHECK-NOT: @__msan_va_arg_overflow_size_tls
+; CHECK: ret void
+
+
+; Test handling of volatile stores.
+; Check that MemorySanitizer does not add a check of the value being stored.
+
+define void @VolatileStore(i32* nocapture %p, i32 %x) nounwind uwtable sanitize_memory {
+entry:
+  store volatile i32 %x, i32* %p, align 4
+  ret void
+}
+
+; CHECK-LABEL: @VolatileStore
+; CHECK: @__msan_warning{{.*}}
+; CHECL: __msan_metadata_ptr_for_store_4
+; CHECK-NOT: @__msan_warning{{.*}}
+; CHECK: ret void
+
+
+; Test that checks are omitted and returned value is always initialized if
+; sanitize_memory attribute is missing.
+
+define i32 @NoSanitizeMemory(i32 %x) uwtable {
+entry:
+  %tobool = icmp eq i32 %x, 0
+  br i1 %tobool, label %if.end, label %if.then
+
+if.then:                                          ; preds = %entry
+  tail call void @bar()
+  br label %if.end
+
+if.end:                                           ; preds = %entry, %if.then
+  ret i32 %x
+}
+
+declare void @bar()
+
+; CHECK-LABEL: @NoSanitizeMemory
+; CHECK-NOT: @__msan_warning{{.*}}
+; CHECK: %retval_shadow
+; CHECK: store i32 0, i32* %_msret
+; CHECK: store i32 0, i32* %retval_origin
+; CHECK-NOT: @__msan_warning{{.*}}
+; CHECK: ret i32
+
+
+; Test that stack allocations are unpoisoned in functions missing
+; sanitize_memory attribute
+
+define i32 @NoSanitizeMemoryAlloca() {
+entry:
+  %p = alloca i32, align 4
+  %x = call i32 @NoSanitizeMemoryAllocaHelper(i32* %p)
+  ret i32 %x
+}
+
+declare i32 @NoSanitizeMemoryAllocaHelper(i32* %p)
+
+; CHECK-LABEL: @NoSanitizeMemoryAlloca
+; CHECK: call i8* @llvm.returnaddress(i32 0)
+; CHECK: call void @__msan_poison_alloca({{.*}}, i64 4, {{.*}})
+; CHECK: call i32 @NoSanitizeMemoryAllocaHelper(i32* 
+; CHECK: ret i32
+
+
+; Test that undef is unpoisoned in functions missing
+; sanitize_memory attribute
+
+define i32 @NoSanitizeMemoryUndef() {
+entry:
+  %x = call i32 @NoSanitizeMemoryUndefHelper(i32 undef)
+  ret i32 %x
+}
+
+declare i32 @NoSanitizeMemoryUndefHelper(i32 %x)
+
+; CHECK-LABEL: @NoSanitizeMemoryUndef
+; CHECK: %param_shadow
+; CHECK: store i32 0
+; CHECK: call i32 @NoSanitizeMemoryUndefHelper(i32 undef)
+; CHECK: ret i32
+
+
+; Test PHINode instrumentation in blacklisted functions
+
+define i32 @NoSanitizeMemoryPHI(i32 %x) {
+entry:
+  %tobool = icmp ne i32 %x, 0
+  br i1 %tobool, label %cond.true, label %cond.false
+
+cond.true:                                        ; preds = %entry
+  br label %cond.end
+
+cond.false:                                       ; preds = %entry
+  br label %cond.end
+
+cond.end:                                         ; preds = %cond.false, %cond.true
+  %cond = phi i32 [ undef, %cond.true ], [ undef, %cond.false ]
+  ret i32 %cond
+}
+
+; CHECK: [[A:%.*]] = phi i32 [ undef, %cond.true ], [ undef, %cond.false ]
+; CHECK: %retval_shadow
+; CHECK: store i32 0
+; CHECK: %retval_origin
+; CHECK: ret i32 [[A]]
+
+
+; Test that there are no __msan_param_origin_tls stores when
+; argument shadow is a compile-time zero constant (which is always the case
+; in functions missing sanitize_memory attribute).
+
+define i32 @NoSanitizeMemoryParamTLS(i32* nocapture readonly %x) {
+entry:
+  %0 = load i32, i32* %x, align 4
+  %call = tail call i32 @NoSanitizeMemoryParamTLSHelper(i32 %0)
+  ret i32 %call
+}
+
+declare i32 @NoSanitizeMemoryParamTLSHelper(i32 %x)
+
+; CHECK-LABEL: define i32 @NoSanitizeMemoryParamTLS(
+; CHECK-NOT: __msan_param_origin_tls
+; CHECK: ret i32
+
+
+; Test argument shadow alignment
+
+define <2 x i64> @ArgumentShadowAlignment(i64 %a, <2 x i64> %b) sanitize_memory {
+entry:
+  ret <2 x i64> %b
+}
+
+; CHECK-LABEL: @ArgumentShadowAlignment
+; CHECK: %param_shadow
+; CHECK: load <2 x i64>, <2 x i64>* {{.*}}, align 8
+; CHECK: %param_origin
+; CHECK: load i32
+; CHECK: %retval_shadow
+; CHECK: store <2 x i64> {{.*}}, <2 x i64>* {{.*}} align 8
+; CHECK: store i32 {{.*}}, i32* %retval_origin
+; CHECK: ret <2 x i64>
+
+
+; Test shadow propagation for aggregates passed through ellipsis.
+
+%struct.StructByVal = type { i32, i32, i32, i32 }
+
+declare void @VAArgStructFn(i32 %guard, ...)
+declare void @llvm.memcpy.p0i8.p0i8.i64(i8* nocapture, i8* nocapture, i64, i32, i1) nounwind
+
+define void @VAArgStruct(%struct.StructByVal* nocapture %s) sanitize_memory {
+entry:
+  %agg.tmp2 = alloca %struct.StructByVal, align 8
+  %0 = bitcast %struct.StructByVal* %s to i8*
+  %agg.tmp.sroa.0.0..sroa_cast = bitcast %struct.StructByVal* %s to i64*
+  %agg.tmp.sroa.0.0.copyload = load i64, i64* %agg.tmp.sroa.0.0..sroa_cast, align 4
+  %agg.tmp.sroa.2.0..sroa_idx = getelementptr inbounds %struct.StructByVal, %struct.StructByVal* %s, i64 0, i32 2
+  %agg.tmp.sroa.2.0..sroa_cast = bitcast i32* %agg.tmp.sroa.2.0..sroa_idx to i64*
+  %agg.tmp.sroa.2.0.copyload = load i64, i64* %agg.tmp.sroa.2.0..sroa_cast, align 4
+  %1 = bitcast %struct.StructByVal* %agg.tmp2 to i8*
+  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %1, i8* %0, i64 16, i32 4, i1 false)
+  call void (i32, ...) @VAArgStructFn(i32 undef, i64 %agg.tmp.sroa.0.0.copyload, i64 %agg.tmp.sroa.2.0.copyload, i64 %agg.tmp.sroa.0.0.copyload, i64 %agg.tmp.sroa.2.0.copyload, %struct.StructByVal* byval align 8 %agg.tmp2)
+  ret void
+}
+
+
+; "undef" and the first 2 structs go to general purpose registers;
+; the third struct goes to the overflow area byval
+
+; CHECK-LABEL: @VAArgStruct
+; undef not stored to %va_arg_shadow - it's a fixed argument
+; first struct through general purpose registers
+; CHECK: ptrtoint {{.*}} %va_arg_shadow
+; CHECK: store i64 {{.*}} align 8
+; CHECK: ptrtoint {{.*}} %va_arg_shadow
+; CHECK: store i64 {{.*}} align 8
+; second struct through general purpose registers
+; CHECK: ptrtoint {{.*}} %va_arg_shadow
+; CHECK: store i64 {{.*}} align 8
+; CHECK: ptrtoint {{.*}} %va_arg_shadow
+; CHECK: store i64 {{.*}} align 8
+; third struct through the overflow area byval
+; CHECK: %va_arg_shadow to i64
+; CHECK: add i64 {{.*}}, 176
+; CHECK: call void @llvm.memcpy.p0i8.p0i8.i64
+; CHECK: store i64 16, i64* %va_arg_overflow_size
+; CHECK: call void (i32, ...) @VAArgStructFn
+; CHECK: ret void
+
Index: test/Instrumentation/MemorySanitizer/msan_userspace_basic.ll
===================================================================
--- test/Instrumentation/MemorySanitizer/msan_userspace_basic.ll	(revision 0)
+++ test/Instrumentation/MemorySanitizer/msan_userspace_basic.ll	(working copy)
@@ -0,0 +1,440 @@
+; RUN: opt < %s -msan -msan-check-access-address=0 -S | FileCheck %s
+; RUN: opt < %s -msan -msan-check-access-address=0 -msan-track-origins=1 -S | FileCheck -check-prefix=CHECK -check-prefix=CHECK-ORIGINS %s
+
+target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
+target triple = "x86_64-unknown-linux-gnu"
+
+; CHECK: @llvm.global_ctors {{.*}} { i32 0, void ()* @msan.module_ctor, i8* null }
+
+; Check the presence and the linkage type of __msan_track_origins and
+; other interface symbols.
+; CHECK-NOT: @__msan_track_origins
+; CHECK-ORIGINS: @__msan_track_origins = weak_odr constant i32 1
+; CHECK-NOT: @__msan_keep_going = weak_odr constant i32 0
+; CHECK: @__msan_retval_tls = external thread_local(initialexec) global [{{.*}}]
+; CHECK: @__msan_retval_origin_tls = external thread_local(initialexec) global i32
+; CHECK: @__msan_param_tls = external thread_local(initialexec) global [{{.*}}]
+; CHECK: @__msan_param_origin_tls = external thread_local(initialexec) global [{{.*}}]
+; CHECK: @__msan_va_arg_tls = external thread_local(initialexec) global [{{.*}}]
+; CHECK: @__msan_va_arg_overflow_size_tls = external thread_local(initialexec) global i64
+; CHECK: @__msan_origin_tls = external thread_local(initialexec) global i32
+
+
+; Check instrumentation of stores
+
+define void @Store(i32* nocapture %p, i32 %x) nounwind uwtable sanitize_memory {
+entry:
+  store i32 %x, i32* %p, align 4
+  ret void
+}
+
+; CHECK-LABEL: @Store
+; CHECK: load {{.*}} @__msan_param_tls
+; CHECK-ORIGINS: load {{.*}} @__msan_param_origin_tls
+; CHECK: store
+; CHECK-ORIGINS: icmp
+; CHECK-ORIGINS: br i1
+; CHECK-ORIGINS: <label>
+; CHECK-ORIGINS: store
+; CHECK-ORIGINS: br label
+; CHECK-ORIGINS: <label>
+; CHECK: store
+; CHECK: ret void
+
+
+; Check instrumentation of aligned stores
+; Shadow store has the same alignment as the original store; origin store
+; does not specify explicit alignment.
+
+define void @AlignedStore(i32* nocapture %p, i32 %x) nounwind uwtable sanitize_memory {
+entry:
+  store i32 %x, i32* %p, align 32
+  ret void
+}
+
+; CHECK-LABEL: @AlignedStore
+; CHECK: load {{.*}} @__msan_param_tls
+; CHECK-ORIGINS: load {{.*}} @__msan_param_origin_tls
+; CHECK: store {{.*}} align 32
+; CHECK-ORIGINS: icmp
+; CHECK-ORIGINS: br i1
+; CHECK-ORIGINS: <label>
+; CHECK-ORIGINS: store {{.*}} align 32
+; CHECK-ORIGINS: br label
+; CHECK-ORIGINS: <label>
+; CHECK: store {{.*}} align 32
+; CHECK: ret void
+
+
+; load followed by cmp: check that we load the shadow and call __msan_warning.
+define void @LoadAndCmp(i32* nocapture %a) nounwind uwtable sanitize_memory {
+entry:
+  %0 = load i32, i32* %a, align 4
+  %tobool = icmp eq i32 %0, 0
+  br i1 %tobool, label %if.end, label %if.then
+
+if.then:                                          ; preds = %entry
+  tail call void (...) @foo() nounwind
+  br label %if.end
+
+if.end:                                           ; preds = %entry, %if.then
+  ret void
+}
+
+declare void @foo(...)
+
+; CHECK-LABEL: @LoadAndCmp
+; CHECK: = load
+; CHECK: = load
+; CHECK: call void @__msan_warning_noreturn()
+; CHECK-NEXT: call void asm sideeffect
+; CHECK-NEXT: unreachable
+; CHECK: ret void
+
+; Check that we store the shadow for the retval.
+define i32 @ReturnInt() nounwind uwtable readnone sanitize_memory {
+entry:
+  ret i32 123
+}
+
+; CHECK-LABEL: @ReturnInt
+; CHECK: store i32 0,{{.*}}__msan_retval_tls
+; CHECK: ret i32
+
+; Check that we get the shadow for the retval.
+define void @CopyRetVal(i32* nocapture %a) nounwind uwtable sanitize_memory {
+entry:
+  %call = tail call i32 @ReturnInt() nounwind
+  store i32 %call, i32* %a, align 4
+  ret void
+}
+
+; CHECK-LABEL: @CopyRetVal
+; CHECK: load{{.*}}__msan_retval_tls
+; CHECK: store
+; CHECK: store
+; CHECK: ret void
+
+
+
+; Check that we propagate shadow for "select"
+
+define i32 @Select(i32 %a, i32 %b, i1 %c) nounwind uwtable readnone sanitize_memory {
+entry:
+  %cond = select i1 %c, i32 %a, i32 %b
+  ret i32 %cond
+}
+
+; CHECK-LABEL: @Select
+; CHECK: select i1
+; CHECK-DAG: or i32
+; CHECK-DAG: xor i32
+; CHECK: or i32
+; CHECK-DAG: select i1
+; CHECK-ORIGINS-DAG: select
+; CHECK-ORIGINS-DAG: select
+; CHECK-DAG: select i1
+; CHECK: store i32{{.*}}@__msan_retval_tls
+; CHECK-ORIGINS: store i32{{.*}}@__msan_retval_origin_tls
+; CHECK: ret i32
+
+
+; Check that we propagate origin for "select" with vector condition.
+; Select condition is flattened to i1, which is then used to select one of the
+; argument origins.
+
+define <8 x i16> @SelectVector(<8 x i16> %a, <8 x i16> %b, <8 x i1> %c) nounwind uwtable readnone sanitize_memory {
+entry:
+  %cond = select <8 x i1> %c, <8 x i16> %a, <8 x i16> %b
+  ret <8 x i16> %cond
+}
+
+; CHECK-LABEL: @SelectVector
+; CHECK: select <8 x i1>
+; CHECK-DAG: or <8 x i16>
+; CHECK-DAG: xor <8 x i16>
+; CHECK: or <8 x i16>
+; CHECK-DAG: select <8 x i1>
+; CHECK-ORIGINS-DAG: select
+; CHECK-ORIGINS-DAG: select
+; CHECK-DAG: select <8 x i1>
+; CHECK: store <8 x i16>{{.*}}@__msan_retval_tls
+; CHECK-ORIGINS: store i32{{.*}}@__msan_retval_origin_tls
+; CHECK: ret <8 x i16>
+
+
+; Check that we propagate origin for "select" with scalar condition and vector
+; arguments. Select condition shadow is sign-extended to the vector type and
+; mixed into the result shadow.
+
+define <8 x i16> @SelectVector2(<8 x i16> %a, <8 x i16> %b, i1 %c) nounwind uwtable readnone sanitize_memory {
+entry:
+  %cond = select i1 %c, <8 x i16> %a, <8 x i16> %b
+  ret <8 x i16> %cond
+}
+
+; CHECK-LABEL: @SelectVector2
+; CHECK: select i1
+; CHECK-DAG: or <8 x i16>
+; CHECK-DAG: xor <8 x i16>
+; CHECK: or <8 x i16>
+; CHECK-DAG: select i1
+; CHECK-ORIGINS-DAG: select i1
+; CHECK-ORIGINS-DAG: select i1
+; CHECK-DAG: select i1
+; CHECK: ret <8 x i16>
+
+
+define { i64, i64 } @SelectStruct(i1 zeroext %x, { i64, i64 } %a, { i64, i64 } %b) readnone sanitize_memory {
+entry:
+  %c = select i1 %x, { i64, i64 } %a, { i64, i64 } %b
+  ret { i64, i64 } %c
+}
+
+; CHECK-LABEL: @SelectStruct
+; CHECK: select i1 {{.*}}, { i64, i64 }
+; CHECK-NEXT: select i1 {{.*}}, { i64, i64 } { i64 -1, i64 -1 }, { i64, i64 }
+; CHECK-ORIGINS: select i1
+; CHECK-ORIGINS: select i1
+; CHECK-NEXT: select i1 {{.*}}, { i64, i64 }
+; CHECK: ret { i64, i64 }
+
+
+define { i64*, double } @SelectStruct2(i1 zeroext %x, { i64*, double } %a, { i64*, double } %b) readnone sanitize_memory {
+entry:
+  %c = select i1 %x, { i64*, double } %a, { i64*, double } %b
+  ret { i64*, double } %c
+}
+
+; CHECK-LABEL: @SelectStruct2
+; CHECK: select i1 {{.*}}, { i64, i64 }
+; CHECK-NEXT: select i1 {{.*}}, { i64, i64 } { i64 -1, i64 -1 }, { i64, i64 }
+; CHECK-ORIGINS: select i1
+; CHECK-ORIGINS: select i1
+; CHECK-NEXT: select i1 {{.*}}, { i64*, double }
+; CHECK: ret { i64*, double }
+
+
+define i8* @IntToPtr(i64 %x) nounwind uwtable readnone sanitize_memory {
+entry:
+  %0 = inttoptr i64 %x to i8*
+  ret i8* %0
+}
+
+; CHECK-LABEL: @IntToPtr
+; CHECK: load i64, i64*{{.*}}__msan_param_tls
+; CHECK-ORIGINS-NEXT: load i32, i32*{{.*}}__msan_param_origin_tls
+; CHECK-NEXT: inttoptr
+; CHECK-NEXT: store i64{{.*}}__msan_retval_tls
+; CHECK: ret i8*
+
+
+define i8* @IntToPtr_ZExt(i16 %x) nounwind uwtable readnone sanitize_memory {
+entry:
+  %0 = inttoptr i16 %x to i8*
+  ret i8* %0
+}
+
+; CHECK-LABEL: @IntToPtr_ZExt
+; CHECK: load i16, i16*{{.*}}__msan_param_tls
+; CHECK: zext
+; CHECK-NEXT: inttoptr
+; CHECK-NEXT: store i64{{.*}}__msan_retval_tls
+; CHECK: ret i8*
+
+
+; Test that va_start instrumentation does not use va_arg_tls*.
+; It should work with a local stack copy instead.
+
+%struct.__va_list_tag = type { i32, i32, i8*, i8* }
+declare void @llvm.va_start(i8*) nounwind
+
+; Function Attrs: nounwind uwtable
+define void @VAStart(i32 %x, ...) sanitize_memory {
+entry:
+  %x.addr = alloca i32, align 4
+  %va = alloca [1 x %struct.__va_list_tag], align 16
+  store i32 %x, i32* %x.addr, align 4
+  %arraydecay = getelementptr inbounds [1 x %struct.__va_list_tag], [1 x %struct.__va_list_tag]* %va, i32 0, i32 0
+  %arraydecay1 = bitcast %struct.__va_list_tag* %arraydecay to i8*
+  call void @llvm.va_start(i8* %arraydecay1)
+  ret void
+}
+
+; CHECK-LABEL: @VAStart
+; CHECK: call void @llvm.va_start
+; CHECK-NOT: @__msan_va_arg_tls
+; CHECK-NOT: @__msan_va_arg_overflow_size_tls
+; CHECK: ret void
+
+
+; Test handling of volatile stores.
+; Check that MemorySanitizer does not add a check of the value being stored.
+
+define void @VolatileStore(i32* nocapture %p, i32 %x) nounwind uwtable sanitize_memory {
+entry:
+  store volatile i32 %x, i32* %p, align 4
+  ret void
+}
+
+; CHECK-LABEL: @VolatileStore
+; CHECK-NOT: @__msan_warning{{.*}}
+; CHECK: ret void
+
+
+; Test that checks are omitted and returned value is always initialized if
+; sanitize_memory attribute is missing.
+
+define i32 @NoSanitizeMemory(i32 %x) uwtable {
+entry:
+  %tobool = icmp eq i32 %x, 0
+  br i1 %tobool, label %if.end, label %if.then
+
+if.then:                                          ; preds = %entry
+  tail call void @bar()
+  br label %if.end
+
+if.end:                                           ; preds = %entry, %if.then
+  ret i32 %x
+}
+
+declare void @bar()
+
+; CHECK-LABEL: @NoSanitizeMemory
+; CHECK-NOT: @__msan_warning{{.*}}
+; CHECK: store i32 0, {{.*}} @__msan_retval_tls
+; CHECK-NOT: @__msan_warning{{.*}}
+; CHECK: ret i32
+
+
+; Test that stack allocations are unpoisoned in functions missing
+; sanitize_memory attribute
+
+define i32 @NoSanitizeMemoryAlloca() {
+entry:
+  %p = alloca i32, align 4
+  %x = call i32 @NoSanitizeMemoryAllocaHelper(i32* %p)
+  ret i32 %x
+}
+
+declare i32 @NoSanitizeMemoryAllocaHelper(i32* %p)
+
+; CHECK-LABEL: @NoSanitizeMemoryAlloca
+; CHECK: call void @llvm.memset.p0i8.i64(i8* {{.*}}, i8 0, i64 4, i32 4, i1 false)
+; CHECK: call i32 @NoSanitizeMemoryAllocaHelper(i32*
+; CHECK: ret i32
+
+
+; Test that undef is unpoisoned in functions missing
+; sanitize_memory attribute
+
+define i32 @NoSanitizeMemoryUndef() {
+entry:
+  %x = call i32 @NoSanitizeMemoryUndefHelper(i32 undef)
+  ret i32 %x
+}
+
+declare i32 @NoSanitizeMemoryUndefHelper(i32 %x)
+
+; CHECK-LABEL: @NoSanitizeMemoryUndef
+; CHECK: store i32 0, i32* {{.*}} @__msan_param_tls
+; CHECK: call i32 @NoSanitizeMemoryUndefHelper(i32 undef)
+; CHECK: ret i32
+
+
+; Test PHINode instrumentation in blacklisted functions
+
+define i32 @NoSanitizeMemoryPHI(i32 %x) {
+entry:
+  %tobool = icmp ne i32 %x, 0
+  br i1 %tobool, label %cond.true, label %cond.false
+
+cond.true:                                        ; preds = %entry
+  br label %cond.end
+
+cond.false:                                       ; preds = %entry
+  br label %cond.end
+
+cond.end:                                         ; preds = %cond.false, %cond.true
+  %cond = phi i32 [ undef, %cond.true ], [ undef, %cond.false ]
+  ret i32 %cond
+}
+
+; CHECK: [[A:%.*]] = phi i32 [ undef, %cond.true ], [ undef, %cond.false ]
+; CHECK: store i32 0, i32* bitcast {{.*}} @__msan_retval_tls
+; CHECK: ret i32 [[A]]
+
+
+; Test that there are no __msan_param_origin_tls stores when
+; argument shadow is a compile-time zero constant (which is always the case
+; in functions missing sanitize_memory attribute).
+
+define i32 @NoSanitizeMemoryParamTLS(i32* nocapture readonly %x) {
+entry:
+  %0 = load i32, i32* %x, align 4
+  %call = tail call i32 @NoSanitizeMemoryParamTLSHelper(i32 %0)
+  ret i32 %call
+}
+
+declare i32 @NoSanitizeMemoryParamTLSHelper(i32 %x)
+
+; CHECK-LABEL: define i32 @NoSanitizeMemoryParamTLS(
+; CHECK-NOT: __msan_param_origin_tls
+; CHECK: ret i32
+
+
+; Test argument shadow alignment
+
+define <2 x i64> @ArgumentShadowAlignment(i64 %a, <2 x i64> %b) sanitize_memory {
+entry:
+  ret <2 x i64> %b
+}
+
+; CHECK-LABEL: @ArgumentShadowAlignment
+; CHECK: load <2 x i64>, <2 x i64>* {{.*}} @__msan_param_tls {{.*}}, align 8
+; CHECK: store <2 x i64> {{.*}} @__msan_retval_tls {{.*}}, align 8
+; CHECK: ret <2 x i64>
+
+
+; Test shadow propagation for aggregates passed through ellipsis.
+
+%struct.StructByVal = type { i32, i32, i32, i32 }
+
+declare void @VAArgStructFn(i32 %guard, ...)
+declare void @llvm.memcpy.p0i8.p0i8.i64(i8* nocapture, i8* nocapture, i64, i32, i1) nounwind
+
+define void @VAArgStruct(%struct.StructByVal* nocapture %s) sanitize_memory {
+entry:
+  %agg.tmp2 = alloca %struct.StructByVal, align 8
+  %0 = bitcast %struct.StructByVal* %s to i8*
+  %agg.tmp.sroa.0.0..sroa_cast = bitcast %struct.StructByVal* %s to i64*
+  %agg.tmp.sroa.0.0.copyload = load i64, i64* %agg.tmp.sroa.0.0..sroa_cast, align 4
+  %agg.tmp.sroa.2.0..sroa_idx = getelementptr inbounds %struct.StructByVal, %struct.StructByVal* %s, i64 0, i32 2
+  %agg.tmp.sroa.2.0..sroa_cast = bitcast i32* %agg.tmp.sroa.2.0..sroa_idx to i64*
+  %agg.tmp.sroa.2.0.copyload = load i64, i64* %agg.tmp.sroa.2.0..sroa_cast, align 4
+  %1 = bitcast %struct.StructByVal* %agg.tmp2 to i8*
+  call void @llvm.memcpy.p0i8.p0i8.i64(i8* %1, i8* %0, i64 16, i32 4, i1 false)
+  call void (i32, ...) @VAArgStructFn(i32 undef, i64 %agg.tmp.sroa.0.0.copyload, i64 %agg.tmp.sroa.2.0.copyload, i64 %agg.tmp.sroa.0.0.copyload, i64 %agg.tmp.sroa.2.0.copyload, %struct.StructByVal* byval align 8 %agg.tmp2)
+  ret void
+}
+
+
+; "undef" and the first 2 structs go to general purpose registers;
+; the third struct goes to the overflow area byval
+
+; CHECK-LABEL: @VAArgStruct
+; undef not stored to __msan_va_arg_tls - it's a fixed argument
+; first struct through general purpose registers
+; CHECK: store i64 {{.*}}, i64* {{.*}}@__msan_va_arg_tls{{.*}}, i64 8){{.*}}, align 8
+; CHECK: store i64 {{.*}}, i64* {{.*}}@__msan_va_arg_tls{{.*}}, i64 16){{.*}}, align 8
+; second struct through general purpose registers
+; CHECK: store i64 {{.*}}, i64* {{.*}}@__msan_va_arg_tls{{.*}}, i64 24){{.*}}, align 8
+; CHECK: store i64 {{.*}}, i64* {{.*}}@__msan_va_arg_tls{{.*}}, i64 32){{.*}}, align 8
+; third struct through the overflow area byval
+; CHECK: ptrtoint %struct.StructByVal* {{.*}} to i64
+; CHECK: bitcast { i32, i32, i32, i32 }* {{.*}}@__msan_va_arg_tls {{.*}}, i64 176
+; CHECK: call void @llvm.memcpy.p0i8.p0i8.i64
+; CHECK: store i64 16, i64* @__msan_va_arg_overflow_size_tls
+; CHECK: call void (i32, ...) @VAArgStructFn
+; CHECK: ret void
+
Index: test/Instrumentation/MemorySanitizer/msan_x86intrinsics.ll
===================================================================
--- test/Instrumentation/MemorySanitizer/msan_x86intrinsics.ll	(revision 320373)
+++ test/Instrumentation/MemorySanitizer/msan_x86intrinsics.ll	(working copy)
@@ -1,5 +1,6 @@
-; RUN: opt < %s -msan -msan-check-access-address=0 -S | FileCheck %s
-; RUN: opt < %s -msan -msan-check-access-address=0 -msan-track-origins=1 -S | FileCheck -check-prefix=CHECK -check-prefix=CHECK-ORIGINS %s
+; RUN: opt < %s -msan -msan-check-access-address=0 -S | FileCheck %s -check-prefix=CHECK -check-prefix=CHECK-MSAN
+; RUN: opt < %s -msan -msan-check-access-address=0 -msan-track-origins=1 -S | FileCheck -check-prefix=CHECK -check-prefix=CHECK-MSAN -check-prefix=CHECK-ORIGINS -check-prefix=CHECK-MSAN-ORIGINS %s
+; RUN: opt < %s -msan -msan-kernel -S | FileCheck -check-prefix=CHECK -check-prefix=CHECK-ORIGINS -check-prefix=CHECK-KMSAN %s
 ; REQUIRES: x86-registered-target
 
 target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
@@ -15,8 +16,8 @@
 declare void @llvm.x86.sse.storeu.ps(i8*, <4 x float>) nounwind
 
 ; CHECK-LABEL: @StoreIntrinsic
-; CHECK-NOT: br
-; CHECK-NOT: = or
+; CHECK-MSAN-NOT: br
+; CHECK-MSAN-NOT: = or
 ; CHECK: store <4 x i32> {{.*}} align 1
 ; CHECK: store <4 x float> %{{.*}}, <4 x float>* %{{.*}}, align 1{{$}}
 ; CHECK: ret void
@@ -34,11 +35,13 @@
 ; CHECK-LABEL: @LoadIntrinsic
 ; CHECK: load <16 x i8>, <16 x i8>* {{.*}} align 1
 ; CHECK-ORIGINS: [[ORIGIN:%[01-9a-z]+]] = load i32, i32* {{.*}}
-; CHECK-NOT: br
-; CHECK-NOT: = or
+; CHECK-MSAN-NOT: br
+; CHECK-MSAN-NOT: = or
 ; CHECK: call <16 x i8> @llvm.x86.sse3.ldu.dq
-; CHECK: store <16 x i8> {{.*}} @__msan_retval_tls
-; CHECK-ORIGINS: store i32 {{.*}}[[ORIGIN]], i32* @__msan_retval_origin_tls
+; CHECK-MSAN: store <16 x i8> {{.*}} @__msan_retval_tls
+; CHECK-KMSAN: %retval_shadow
+; CHECK-KMSAN: store <16 x i8>
+; CHECK-ORIGINS: store i32 {{.*}}[[ORIGIN]], i32* {{.*}}retval_origin{{.*}}
 ; CHECK: ret <16 x i8>
 
 
@@ -53,16 +56,35 @@
 
 declare <8 x i16> @llvm.x86.sse2.padds.w(<8 x i16> %a, <8 x i16> %b) nounwind
 
-; CHECK-LABEL: @Paddsw128
-; CHECK-NEXT: load <8 x i16>, <8 x i16>* {{.*}} @__msan_param_tls
-; CHECK-ORIGINS: load i32, i32* {{.*}} @__msan_param_origin_tls
-; CHECK-NEXT: load <8 x i16>, <8 x i16>* {{.*}} @__msan_param_tls
-; CHECK-ORIGINS: load i32, i32* {{.*}} @__msan_param_origin_tls
-; CHECK-NEXT: = or <8 x i16>
-; CHECK-ORIGINS: = bitcast <8 x i16> {{.*}} to i128
-; CHECK-ORIGINS-NEXT: = icmp ne i128 {{.*}}, 0
-; CHECK-ORIGINS-NEXT: = select i1 {{.*}}, i32 {{.*}}, i32
-; CHECK-NEXT: call <8 x i16> @llvm.x86.sse2.padds.w
-; CHECK-NEXT: store <8 x i16> {{.*}} @__msan_retval_tls
-; CHECK-ORIGINS: store i32 {{.*}} @__msan_retval_origin_tls
-; CHECK-NEXT: ret <8 x i16>
+; CHECK-MSAN-LABEL: @Paddsw128
+; CHECK-MSAN-NEXT: load <8 x i16>, <8 x i16>* {{.*}} @__msan_param_tls
+; CHECK-MSAN-ORIGINS: load i32, i32* {{.*}} @__msan_param_origin_tls
+; CHECK-MSAN-NEXT: load <8 x i16>, <8 x i16>* {{.*}} @__msan_param_tls
+; CHECK-MSAN-ORIGINS: load i32, i32* {{.*}} @__msan_param_origin_tls
+; CHECK-MSAN-NEXT: = or <8 x i16>
+; CHECK-MSAN-ORIGINS: = bitcast <8 x i16> {{.*}} to i128
+; CHECK-MSAN-ORIGINS-NEXT: = icmp ne i128 {{.*}}, 0
+; CHECK-MSAN-ORIGINS-NEXT: = select i1 {{.*}}, i32 {{.*}}, i32
+; CHECK-MSAN-NEXT: call <8 x i16> @llvm.x86.sse2.padds.w
+; CHECK-MSAN-NEXT: store <8 x i16> {{.*}} @__msan_retval_tls
+; CHECK-MSAN-ORIGINS: store i32 {{.*}} @__msan_retval_origin_tls
+; CHECK-MSAN-NEXT: ret <8 x i16>
+
+; CHECK-KMSAN-LABEL: @Paddsw128
+; CHECK-KMSAN: %param_shadow
+; CHECK-KMSAN: load <8 x i16>, <8 x i16>*
+; CHECK-KMSAN: %param_origin
+; CHECK-KMSAN: load i32, i32*
+; CHECK-KMSAN: %param_shadow
+; CHECK-KMSAN: load <8 x i16>, <8 x i16>*
+; CHECK-KMSAN: %param_origin
+; CHECK-KMSAN: load i32, i32*
+; CHECK-KMSAN-NEXT: = or <8 x i16>
+; CHECK-KMSAN: = bitcast <8 x i16> {{.*}} to i128
+; CHECK-KMSAN-NEXT: = icmp ne i128 {{.*}}, 0
+; CHECK-KMSAN-NEXT: = select i1 {{.*}}, i32 {{.*}}, i32
+; CHECK-KMSAN-NEXT: call <8 x i16> @llvm.x86.sse2.padds.w
+; CHECK-KMSAN-NEXT: %retval_shadow
+; CHECK-KMSAN <8 x i16> {{.*}} <8 x i16>*
+; CHECK-KMSAN: store i32 {{.*}} %retval_origin
+; CHECK-KMSAN-NEXT: ret <8 x i16>
